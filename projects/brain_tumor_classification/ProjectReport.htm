<html>

<head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <style type="text/css">
        ol.lst-kix_list_1-3 {
            list-style-type: none
        }

        ol.lst-kix_list_1-4 {
            list-style-type: none
        }

        .lst-kix_list_2-6>li:before {
            content: "" counter(lst-ctn-kix_list_2-0, decimal) "." counter(lst-ctn-kix_list_2-1, decimal) "." counter(lst-ctn-kix_list_2-2, decimal) "." counter(lst-ctn-kix_list_2-3, decimal) "." counter(lst-ctn-kix_list_2-4, decimal) "." counter(lst-ctn-kix_list_2-5, decimal) "." counter(lst-ctn-kix_list_2-6, decimal) " "
        }

        .lst-kix_list_2-7>li:before {
            content: "" counter(lst-ctn-kix_list_2-0, decimal) "." counter(lst-ctn-kix_list_2-1, decimal) "." counter(lst-ctn-kix_list_2-2, decimal) "." counter(lst-ctn-kix_list_2-3, decimal) "." counter(lst-ctn-kix_list_2-4, decimal) "." counter(lst-ctn-kix_list_2-5, decimal) "." counter(lst-ctn-kix_list_2-6, decimal) "." counter(lst-ctn-kix_list_2-7, decimal) " "
        }

        .lst-kix_list_2-7>li {
            counter-increment: lst-ctn-kix_list_2-7
        }

        ol.lst-kix_list_1-5 {
            list-style-type: none
        }

        ol.lst-kix_list_1-6 {
            list-style-type: none
        }

        .lst-kix_list_2-1>li {
            counter-increment: lst-ctn-kix_list_2-1
        }

        ol.lst-kix_list_1-0 {
            list-style-type: none
        }

        .lst-kix_list_2-4>li:before {
            content: "" counter(lst-ctn-kix_list_2-0, decimal) "." counter(lst-ctn-kix_list_2-1, decimal) "." counter(lst-ctn-kix_list_2-2, decimal) "." counter(lst-ctn-kix_list_2-3, decimal) "." counter(lst-ctn-kix_list_2-4, decimal) " "
        }

        .lst-kix_list_2-5>li:before {
            content: "" counter(lst-ctn-kix_list_2-0, decimal) "." counter(lst-ctn-kix_list_2-1, decimal) "." counter(lst-ctn-kix_list_2-2, decimal) "." counter(lst-ctn-kix_list_2-3, decimal) "." counter(lst-ctn-kix_list_2-4, decimal) "." counter(lst-ctn-kix_list_2-5, decimal) " "
        }

        .lst-kix_list_2-8>li:before {
            content: "" counter(lst-ctn-kix_list_2-0, decimal) "." counter(lst-ctn-kix_list_2-1, decimal) "." counter(lst-ctn-kix_list_2-2, decimal) "." counter(lst-ctn-kix_list_2-3, decimal) "." counter(lst-ctn-kix_list_2-4, decimal) "." counter(lst-ctn-kix_list_2-5, decimal) "." counter(lst-ctn-kix_list_2-6, decimal) "." counter(lst-ctn-kix_list_2-7, decimal) "." counter(lst-ctn-kix_list_2-8, decimal) " "
        }

        ol.lst-kix_list_1-1 {
            list-style-type: none
        }

        ol.lst-kix_list_1-2 {
            list-style-type: none
        }

        ol.lst-kix_list_3-0 {
            list-style-type: none
        }

        .lst-kix_list_1-1>li {
            counter-increment: lst-ctn-kix_list_1-1
        }

        ol.lst-kix_list_2-6.start {
            counter-reset: lst-ctn-kix_list_2-6 0
        }

        .lst-kix_list_3-0>li:before {
            content: "[" counter(lst-ctn-kix_list_3-0, decimal) "] "
        }

        .lst-kix_list_3-1>li:before {
            content: " "
        }

        .lst-kix_list_3-2>li:before {
            content: " "
        }

        ul.lst-kix_list_3-7 {
            list-style-type: none
        }

        ul.lst-kix_list_3-8 {
            list-style-type: none
        }

        ol.lst-kix_list_1-8.start {
            counter-reset: lst-ctn-kix_list_1-8 0
        }

        ol.lst-kix_list_2-3.start {
            counter-reset: lst-ctn-kix_list_2-3 0
        }

        ol.lst-kix_list_3-0.start {
            counter-reset: lst-ctn-kix_list_3-0 0
        }

        ul.lst-kix_list_3-1 {
            list-style-type: none
        }

        .lst-kix_list_3-5>li:before {
            content: " "
        }

        ul.lst-kix_list_3-2 {
            list-style-type: none
        }

        .lst-kix_list_3-4>li:before {
            content: " "
        }

        ol.lst-kix_list_1-5.start {
            counter-reset: lst-ctn-kix_list_1-5 0
        }

        ol.lst-kix_list_1-7 {
            list-style-type: none
        }

        .lst-kix_list_3-3>li:before {
            content: " "
        }

        ul.lst-kix_list_3-5 {
            list-style-type: none
        }

        .lst-kix_list_1-7>li {
            counter-increment: lst-ctn-kix_list_1-7
        }

        ol.lst-kix_list_1-8 {
            list-style-type: none
        }

        ul.lst-kix_list_3-6 {
            list-style-type: none
        }

        ul.lst-kix_list_3-3 {
            list-style-type: none
        }

        ul.lst-kix_list_3-4 {
            list-style-type: none
        }

        ol.lst-kix_list_2-5.start {
            counter-reset: lst-ctn-kix_list_2-5 0
        }

        .lst-kix_list_3-8>li:before {
            content: " "
        }

        .lst-kix_list_2-0>li {
            counter-increment: lst-ctn-kix_list_2-0
        }

        .lst-kix_list_2-3>li {
            counter-increment: lst-ctn-kix_list_2-3
        }

        .lst-kix_list_2-6>li {
            counter-increment: lst-ctn-kix_list_2-6
        }

        .lst-kix_list_3-6>li:before {
            content: " "
        }

        .lst-kix_list_3-7>li:before {
            content: " "
        }

        ol.lst-kix_list_1-7.start {
            counter-reset: lst-ctn-kix_list_1-7 0
        }

        .lst-kix_list_1-2>li {
            counter-increment: lst-ctn-kix_list_1-2
        }

        ol.lst-kix_list_2-2.start {
            counter-reset: lst-ctn-kix_list_2-2 0
        }

        .lst-kix_list_1-5>li {
            counter-increment: lst-ctn-kix_list_1-5
        }

        .lst-kix_list_1-8>li {
            counter-increment: lst-ctn-kix_list_1-8
        }

        ol.lst-kix_list_1-4.start {
            counter-reset: lst-ctn-kix_list_1-4 0
        }

        ol.lst-kix_list_1-1.start {
            counter-reset: lst-ctn-kix_list_1-1 0
        }

        ol.lst-kix_list_2-2 {
            list-style-type: none
        }

        ol.lst-kix_list_2-3 {
            list-style-type: none
        }

        ol.lst-kix_list_2-4 {
            list-style-type: none
        }

        ol.lst-kix_list_2-5 {
            list-style-type: none
        }

        .lst-kix_list_1-4>li {
            counter-increment: lst-ctn-kix_list_1-4
        }

        ol.lst-kix_list_2-0 {
            list-style-type: none
        }

        .lst-kix_list_2-4>li {
            counter-increment: lst-ctn-kix_list_2-4
        }

        ol.lst-kix_list_1-6.start {
            counter-reset: lst-ctn-kix_list_1-6 0
        }

        ol.lst-kix_list_2-1 {
            list-style-type: none
        }

        ol.lst-kix_list_1-3.start {
            counter-reset: lst-ctn-kix_list_1-3 0
        }

        ol.lst-kix_list_2-8.start {
            counter-reset: lst-ctn-kix_list_2-8 0
        }

        ol.lst-kix_list_1-2.start {
            counter-reset: lst-ctn-kix_list_1-2 0
        }

        .lst-kix_list_1-0>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) ". "
        }

        ol.lst-kix_list_2-6 {
            list-style-type: none
        }

        .lst-kix_list_1-1>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) "." counter(lst-ctn-kix_list_1-1, decimal) ". "
        }

        .lst-kix_list_1-2>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) "." counter(lst-ctn-kix_list_1-1, decimal) "." counter(lst-ctn-kix_list_1-2, decimal) " "
        }

        ol.lst-kix_list_2-0.start {
            counter-reset: lst-ctn-kix_list_2-0 0
        }

        ol.lst-kix_list_2-7 {
            list-style-type: none
        }

        ol.lst-kix_list_2-8 {
            list-style-type: none
        }

        .lst-kix_list_1-3>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) "." counter(lst-ctn-kix_list_1-1, decimal) "." counter(lst-ctn-kix_list_1-2, decimal) "." counter(lst-ctn-kix_list_1-3, decimal) " "
        }

        .lst-kix_list_1-4>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) "." counter(lst-ctn-kix_list_1-1, decimal) "." counter(lst-ctn-kix_list_1-2, decimal) "." counter(lst-ctn-kix_list_1-3, decimal) "." counter(lst-ctn-kix_list_1-4, decimal) " "
        }

        ol.lst-kix_list_1-0.start {
            counter-reset: lst-ctn-kix_list_1-0 0
        }

        .lst-kix_list_1-0>li {
            counter-increment: lst-ctn-kix_list_1-0
        }

        .lst-kix_list_3-0>li {
            counter-increment: lst-ctn-kix_list_3-0
        }

        .lst-kix_list_1-6>li {
            counter-increment: lst-ctn-kix_list_1-6
        }

        .lst-kix_list_1-7>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) "." counter(lst-ctn-kix_list_1-1, decimal) "." counter(lst-ctn-kix_list_1-2, decimal) "." counter(lst-ctn-kix_list_1-3, decimal) "." counter(lst-ctn-kix_list_1-4, decimal) "." counter(lst-ctn-kix_list_1-5, decimal) "." counter(lst-ctn-kix_list_1-6, decimal) "." counter(lst-ctn-kix_list_1-7, decimal) " "
        }

        ol.lst-kix_list_2-7.start {
            counter-reset: lst-ctn-kix_list_2-7 0
        }

        .lst-kix_list_1-3>li {
            counter-increment: lst-ctn-kix_list_1-3
        }

        .lst-kix_list_1-5>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) "." counter(lst-ctn-kix_list_1-1, decimal) "." counter(lst-ctn-kix_list_1-2, decimal) "." counter(lst-ctn-kix_list_1-3, decimal) "." counter(lst-ctn-kix_list_1-4, decimal) "." counter(lst-ctn-kix_list_1-5, decimal) " "
        }

        .lst-kix_list_1-6>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) "." counter(lst-ctn-kix_list_1-1, decimal) "." counter(lst-ctn-kix_list_1-2, decimal) "." counter(lst-ctn-kix_list_1-3, decimal) "." counter(lst-ctn-kix_list_1-4, decimal) "." counter(lst-ctn-kix_list_1-5, decimal) "." counter(lst-ctn-kix_list_1-6, decimal) " "
        }

        li.li-bullet-0:before {
            margin-left: -21.6pt;
            white-space: nowrap;
            display: inline-block;
            min-width: 21.6pt
        }

        .lst-kix_list_2-0>li:before {
            content: "" counter(lst-ctn-kix_list_2-0, decimal) ". "
        }

        .lst-kix_list_2-1>li:before {
            content: "" counter(lst-ctn-kix_list_2-0, decimal) "." counter(lst-ctn-kix_list_2-1, decimal) ". "
        }

        ol.lst-kix_list_2-1.start {
            counter-reset: lst-ctn-kix_list_2-1 0
        }

        .lst-kix_list_2-5>li {
            counter-increment: lst-ctn-kix_list_2-5
        }

        .lst-kix_list_2-8>li {
            counter-increment: lst-ctn-kix_list_2-8
        }

        .lst-kix_list_1-8>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) "." counter(lst-ctn-kix_list_1-1, decimal) "." counter(lst-ctn-kix_list_1-2, decimal) "." counter(lst-ctn-kix_list_1-3, decimal) "." counter(lst-ctn-kix_list_1-4, decimal) "." counter(lst-ctn-kix_list_1-5, decimal) "." counter(lst-ctn-kix_list_1-6, decimal) "." counter(lst-ctn-kix_list_1-7, decimal) "." counter(lst-ctn-kix_list_1-8, decimal) " "
        }

        .lst-kix_list_2-2>li:before {
            content: "" counter(lst-ctn-kix_list_2-0, decimal) "." counter(lst-ctn-kix_list_2-1, decimal) "." counter(lst-ctn-kix_list_2-2, decimal) " "
        }

        .lst-kix_list_2-3>li:before {
            content: "" counter(lst-ctn-kix_list_2-0, decimal) "." counter(lst-ctn-kix_list_2-1, decimal) "." counter(lst-ctn-kix_list_2-2, decimal) "." counter(lst-ctn-kix_list_2-3, decimal) " "
        }

        .lst-kix_list_2-2>li {
            counter-increment: lst-ctn-kix_list_2-2
        }

        ol.lst-kix_list_2-4.start {
            counter-reset: lst-ctn-kix_list_2-4 0
        }

        ol {
            margin: 0;
            padding: 0
        }

        table td,
        table th {
            padding: 0
        }

        .c29 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 61.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c25 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 63pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c36 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 65.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c15 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 63.8pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c9 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 46.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c6 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 67.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c0 {
            color: #000000;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 13pt;
           
            font-style: normal
        }

        .c2 {
            color: #000000;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 11ptpt;
           
            font-style: normal
        }

        .c43 {
            margin-left: 18pt;
            padding-top: 9pt;
            text-indent: -18pt;
            padding-bottom: 9pt;
            line-height: 1.0;
            
        }

        .c13 {
            padding-top: 12.5pt;
            text-indent: 12pt;
            padding-bottom: 0pt;
            line-height: 0.999603807926178;
           
            margin-right: 11.4pt
        }

        .c18 {
            margin-left: 18pt;
            padding-top: 0pt;
            text-indent: -18pt;
            padding-bottom: 9pt;
            line-height: 1.0;
            
        }

        .c12 {
            padding-top: 0.9pt;
            padding-bottom: 10pt;
            line-height: 0.999599277973175;
           
            margin-right: 11.7pt;
            height: 10pt
        }

        .c17 {
            padding-top: 0.9pt;
            padding-bottom: 10pt;
            line-height: 0.999599277973175;
           
            margin-right: 11.7pt
        }

        .c31 {
            padding-top: 7.8pt;
            padding-bottom: 0pt;
            line-height: 0.9996072053909302;
           
            margin-right: 11.8pt
        }

        .c62 {
            padding-top: 24pt;
            padding-bottom: 6pt;
            line-height: 1.0;
            page-break-after: avoid;
            text-align: center
        }

        .c41 {
            padding-top: 11.6pt;
            padding-bottom: 0pt;
            line-height: 0.9996004700660706;
           
            margin-right: 11.8pt
        }

        .c8 {
            padding-top: 0.9pt;
            padding-bottom: 10pt;
            line-height: 0.999599277973175;
            text-align: center;
            margin-right: 11.7pt
        }

        .c26 {
            padding-top: 0.9pt;
            padding-bottom: 0pt;
            line-height: 0.999599277973175;
           
            margin-right: 11.7pt
        }

        .c20 {
            padding-top: 16.7pt;
            padding-bottom: 0pt;
            line-height: 0.9996324181556702;
            text-align: center;
            margin-right: 1pt
        }

        .c35 {
            padding-top: 16.7pt;
            padding-bottom: 10pt;
            line-height: 0.9996324181556702;
            text-align: center;
            margin-right: 1pt
        }

        .c32 {
            padding-top: 7.8pt;
            padding-bottom: 10pt;
            line-height: 0.9996072053909302;
           
            margin-right: 11.8pt
        }

        .c55 {
            padding-top: 0pt;
            text-indent: 10.1pt;
            padding-bottom: 0pt;
            line-height: 1.0;
            
        }

        .c52 {
            padding-top: 12pt;
            padding-bottom: 4pt;
            line-height: 1.0;
            page-break-after: avoid;
            text-align: left
        }

        .c44 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 0.9996324181556702;
           
            margin-right: 1pt
        }

        .c50 {
            padding-top: 0pt;
            padding-bottom: 10pt;
            line-height: 0.9996324181556702;
            text-align: center;
            margin-right: 1pt
        }

        .c59 {
            padding-top: 1.1pt;
            padding-bottom: 0pt;
            line-height: 0.9995900392532349;
           
            margin-right: 0.9pt
        }

        .c14 {
            padding-top: 16.7pt;
            padding-bottom: 0pt;
            line-height: 0.9996324181556702;
           
            margin-right: 1pt
        }

        .c46 {
            padding-top: 0pt;
            padding-bottom: 10pt;
            line-height: 0.9996004700660706;
           
            margin-right: 11.8pt
        }

        .c21 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.0181047916412354;
            text-align: center;
            margin-right: 1pt
        }

        .c53 {
            padding-top: 7.6pt;
            padding-bottom: 0pt;
            line-height: 0.9996067881584167;
           
            margin-right: 11.7pt
        }

        .c38 {
            padding-top: 16.7pt;
            padding-bottom: 10pt;
            line-height: 0.9996324181556702;
           
            margin-right: 1pt
        }

        .c24 {
            padding-top: 7.6pt;
            padding-bottom: 0pt;
            line-height: 0.9996064901351929;
           
            margin-right: 1pt
        }

        .c60 {
            padding-top: 7.8pt;
            padding-bottom: 0pt;
            line-height: 0.9996042251586914;
           
            margin-right: 0.9pt
        }

        .c51 {
            margin-left: auto;
            border-spacing: 0;
            border-collapse: collapse;
            margin-right: auto
        }

        .c4 {
            vertical-align: baseline;
            font-size: 13pt;
            font-family: "Arial";
            font-style: normal
        }

        .c39 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.0;
            text-align: left
        }

        .c19 {
            vertical-align: baseline;
            font-size: 8pt;
           
            font-style: normal
        }

        .c1 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.0;
            text-align: center
        }

        .c16 {
            vertical-align: baseline;
            font-size: 11ptpt;
           
            font-style: normal
        }

        .c63 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            text-align: left
        }

        .c42 {
            vertical-align: baseline;
            font-size: 7pt;
           
            font-style: normal
        }

        .c37 {
            vertical-align: baseline;
            font-size: 20pt;
           
            font-style: normal
        }

        .c30 {
            vertical-align: baseline;
            font-size: 11pt;
           
            font-style: normal
        }

        .c3 {
            color: #000000;
            font-weight: 400;
            text-decoration: none
        }

        .c33 {
            font-size: 20pt;
            font-family: "Arial";
            font-weight: 400
        }

        .c10 {
            color: #000000;
            /* font-weight: 700; */
            text-decoration: none
        }

        .c56 {
            background-color: #ffffff;
            max-width: 494.9pt;
            padding: 72pt 45.1pt 81.1pt 72pt
        }

        .c57 {
            padding: 0;
            margin: 0
        }

        .c23 {
            orphans: 2;
            widows: 2
        }

        .c49 {
            font-size: 6pt;
            font-style: normal
        }

        .c28 {
            font-size: 20pt;
            /* font-weight: 700 */
        }

        .c47 {
            vertical-align: baseline;
            
        }

        .c5 {
            color: inherit;
            text-decoration: inherit
        }

        .c48 {
            font-size: 11ptpt;
            font-style: italic
        }

        .c58 {
            margin-left: 18pt;
            padding-left: 3.6pt
        }

        .c34 {
            font-size: 11pt;
            font-weight: 700
        }

        .c11 {
            font-size: 20pt;
            /* font-style: italic */
        }

        .c54 {
            /* font-style: italic */
        }

        .c40 {
            /* font-weight: 700 */
        }

        .c22 {
            height: 0pt
        }

        .c7 {
            font-size: 13pt
        }

        .c45 {
            margin-left: 0.1pt
        }

        .c27 {
            height: 10pt
        }

        .c61 {
            font-style: normal
        }        

        .title {
            padding-top: 24pt;
            color: #000000;
            font-weight: 700;
            font-size: 36pt;
            padding-bottom: 6pt;
           
            line-height: 1.0;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: center
        }

        .subtitle {
            padding-top: 18pt;
            color: #666666;
            font-size: 20pt;
            padding-bottom: 4pt;
            font-family: "Georgia";
            line-height: 1.0;
            page-break-after: avoid;
            font-style: italic;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        li {
            color: #000000;
            font-size: 11ptpt;
            
        }

        p {
            margin-bottom: 10;
            color: #000000;
            font-size: 11ptpt;
        }

        h1 {
            padding-top: 12pt;
            color: #000000;
            font-size: 20pt;
            padding-bottom: 4pt;
           
            line-height: 1.0;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: center
        }

        h2 {
            padding-top: 12pt;
            color: #000000;
            font-size: 11ptpt;
            padding-bottom: 6pt;
           
            line-height: 1.0;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        h3 {
            padding-top: 0pt;
            color: #000000;
            font-size: 11ptpt;
            padding-bottom: 0pt;
           
            line-height: 1.0;
            page-break-after: avoid;
            font-style: italic;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        h4 {
            padding-top: 12pt;
            color: #000000;
            font-size: 11ptpt;
            padding-bottom: 3pt;
           
            line-height: 1.0;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        h5 {
            padding-top: 12pt;
            color: #000000;
            font-size: 13pt;
            padding-bottom: 3pt;
           
            line-height: 1.0;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        h6 {
            padding-top: 12pt;
            color: #000000;
            font-size: 11ptpt;
            padding-bottom: 3pt;
           
            line-height: 1.0;
            orphans: 2;
            widows: 2;
            text-align: left
        }
        * {
            font-family: Arial, Helvetica, sans-serif;
        }

        @media (max-width: 991.98px) {
            p span {
                width: 80% !important;
                height: auto !important;
            }

            img {
                width: 80% !important;
                height: auto !important;
            }

            .c628 {
                width: 30% !important;
            }
        }
    </style>
</head>

<body class="c56 doc-content">
    <div>
        <p class="c39 c23 c27"><span class="c2"></span></p>
    </div>
    <p class="c62 c23 title" id="h.q6v84hr3s2gy"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 256.07px; height: 265.55px;"><img
                alt="" src="images/image3.png"
                style="width: 256.07px; height: 265.55px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c1 c23 c27"><span class="c37 c10"></span></p>
    <p class="c23"><span class="c28">Abstract</span></p>
    <p class="c13"><span class="c54"> Clinicians and doctors detect and classify tumors
            using Magnetic Resonance Imaging (MRI). They can miss-classify a tumor because of
            human errors and biases. Deep learning techniques can help to solve this problem. Especially since computer vision has
            become popular and shown satisfactory performance in image classification tasks. Existing solutions heavily
            rely on different image processing and deep learning methods and acquired acceptable accuracy in
            classification. In this project, we explored applying different deep learning techniques, particularly
            Convolutional Neural Network (CNN), UNet and Transfer learning on the dataset to classify three types of
            brain tumor (meningioma, glioma and pituitary tumor) from MRI data. The dataset consists of 3064 images from
            233 patients. We evaluated the trained model using precision, recall and F1 metrics so that we could compare
            the applied techniques and develop an understanding on how the models generalize on the unseen MRI
            data.</span><span class="c33 c54">&nbsp;</span></p>
    <p class="c23 c27 c55"><span class="c2"></span></p>
    <p class="c17"><span class="c37 c10">1. &nbsp; &nbsp;Introduction</span></p>
    <p class="c45 c53"><span>&nbsp; &nbsp; </span><span>Digital images are increasingly used for diagnosis in medical
            fields </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/OBP1&amp;sa=D&amp;source=editors&amp;ust=1658367235314963&amp;usg=AOvVaw2zuSFXfFBT-L5YOPRwjFBk">[1]</a></span><span>.
            With better imaging technologies, more information can be analyzed for better diagnoses. Yet, human bias and
            error can still contribute to misdiagnoses. A machine learning model can learn to identify features in
            images and aid the diagnostician in finding the correct diagnoses </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/tGM1&amp;sa=D&amp;source=editors&amp;ust=1658367235315266&amp;usg=AOvVaw0405mwLGvvL88NGSQcAsJn">[2]</a></span><span
            class="c2">. </span></p>
    <p class="c26 c45"><span class="c2">&nbsp; &nbsp; Brain tumor detection and classification is usually done by human
            inspection of a highly trained radiologist. The data most commonly used for detecting brain tumors is
            Nuclear Magnetic Resonance Imaging (MRI) data. Radiologists look for specific characteristics in the MRI
            image to identify if there is a brain tumor and if so, which type of tumor. Several image processing
            techniques have been used to help these radiologists identify said features. We have proposed using deep
            learning techniques trained on enhanc</span></p>
    <p class="c26 c45"><span class="c2">ed MRI data to aid in the classification of brain tumor types. </span></p>
    <p class="c26"><span class="c2">&nbsp; &nbsp; Our project focused on investigating different deep learning
            techniques for extracting information from the MRI data. Specifically, we look at vectorizing the enhanced
            MRI images with UNet, classifying tumor types using CNNs, and attempting to use transfer learning on more
            complex pretrained models for tumor classification.</span></p>
    <p class="c26 c27"><span class="c2"></span></p>
    <p class="c17"><span class="c37 c10">2. &nbsp; &nbsp;Related Work</span></p>
    <p class="c60"><span>&nbsp; &nbsp; </span><span>Over the years, there has been much work on brain tumor
            classification using different techniques of deep learning. Some of the notable works are mentioned in this
            section. Recently in 2022, In a paper, a novel hierarchical deep learning-based brain tumor (HDL2BT),
            classification is proposed using CNN. The model used a total of 3264 images for the glioma, meningioma, and
            pituitary and no tumor class. Following tumor detection, the system divides the tumor into different
            classifications </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/WN5aI&amp;sa=D&amp;source=editors&amp;ust=1658367235315857&amp;usg=AOvVaw22Y3drnXIQhJc0XIPs5mMO">[3]</a></span><span>.
            Another study in 2021 proposed utilizing a deep learning architecture called the EfficientNet-B3 model to
            automatically classify brain tumors in MRI data. This model can decipher MRI images with three different
            types of cancers: meningioma, glioma, and pituitary tumors </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/9d8n&amp;sa=D&amp;source=editors&amp;ust=1658367235316071&amp;usg=AOvVaw3w4qoTiom7hXgosf_SL4Rt">[4]</a></span><span>.
            In another the paper, the researchers provided a fully automatic brain tumor segmentation and classification
            model with a multiscale approach utilizing a Deep Convolutional Neural Network which can evaluate MRI scans
            with three types of tumors: meningioma, glioma, and pituitary tumor, in sagittal, coronal, and axial
            perspectives, and does not require preprocessing of input images to remove skull or vertebral column
            components </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/zy2f&amp;sa=D&amp;source=editors&amp;ust=1658367235316280&amp;usg=AOvVaw33wgnZRR0Co42LNPIve9ea">[5]</a></span><span>.
            Another paper proposed three different CNN models for three separate categorization tasks. The first CNN
            model detects brain tumors with a 99.33% accuracy rate. The second CNN model can classify the brain tumor
            into five types: normal, glioma, meningioma, pituitary, and metastatic and the third CNN model can
            categorize brain tumors into three grades: Grade II, Grade III, and Grade IV. This is the first study to use
            CNN for multi-classification of brain tumor MRI images, with the grid search optimizer tuning practically
            all hyper-parameters </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/Mmgv&amp;sa=D&amp;source=editors&amp;ust=1658367235316518&amp;usg=AOvVaw2lB-gkV0ZpRwoIH-5_G72h">[6]</a></span><span
            class="c2">. </span></p>
    <p class="c59"><span>&nbsp; &nbsp; In another study, they presented a three-class deep learning model for
            categorizing Glioma, Meningioma, and Pituitary tumors but did not include no-tumor. They employed a
            pre-trained InceptionV3 model and applied transfer learn-ing that extracts features from brain MRI images
            and classifies them using the softmax classifier method. The proposed approach outperforms all existing
            methods in a patient-level five-fold cross-validation process on the CE-MRI dataset from figshare
        </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/SSvs&amp;sa=D&amp;source=editors&amp;ust=1658367235316856&amp;usg=AOvVaw2pn48983ZLyXo0GXSp0Gwa">[7]</a></span><span>.
            In another paper the researchers have augmented tumor region via image dilation which is used as the ROI and
            then that region is split into increasingly fine ring-form subregions and then evaluated. The efficacy of
            the proposed method on a large dataset with three feature extraction methods, namely, intensity histogram,
            gray level co-occurrence matrix (GLCM), and bag-of-words (BoW) model </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/IsN7&amp;sa=D&amp;source=editors&amp;ust=1658367235317229&amp;usg=AOvVaw2GemAtRxNvnywBXOGFqKHv">[8]</a></span><span>.
            Lastly, there has been research that developed a Content-based image retrieval (CBIR) system for retrieving
            brain tumors in T1-weighted contrast-enhanced MRI image and proposed a novel feature extraction framework to
            improve the retrieval performance </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/lTF7&amp;sa=D&amp;source=editors&amp;ust=1658367235317589&amp;usg=AOvVaw3Lvhxw4eA088i413PHPFPL">[9]</a></span><span>.
        </span></p>
    <p class="c26 c27"><span class="c10 c37"></span></p>
    <p class="c17"><span class="c37 c10">3. &nbsp; &nbsp;Datasets and Metrics for Experiments</span></p>
    <p class="c24"><span>&nbsp; &nbsp; </span><span>The dataset used in this project is from the Brian Tumor Image
            Dataset on Kaggle </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/nxk4&amp;sa=D&amp;source=editors&amp;ust=1658367235318134&amp;usg=AOvVaw2Yny86D7kn1d1BkOvFWKDO">[10]</a></span><span
            class="c2">&nbsp;gathered by Southern Medical University, Guangzhou, China. The dataset contains 3064
            T1-weighted contrast-enhanced MRI images from 233 patients. The samples are of meningioma [708 slices],
            glioma [1426 slices], and pituitary tumor [930 slices] and are labeled as such.</span></p>
    <p class="c24 c27"><span class="c2"></span></p>
    <p class="c21"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 98.99px; height: 98.99px;"><img
                class="c628  m-2"    
                alt="" src="images/image16.png"
                style="width: 98.99px; height: 98.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 98.99px; height: 98.99px;"><img
                class="c628  m-2"    
                alt="" src="images/image4.png"
                style="width: 98.99px; height: 98.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 98.99px; height: 98.99px;"><img
                class="c628 m-2"    
                alt="" src="images/image10.png"
                style="width: 98.99px; height: 98.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c21 c27"><span class="c0"></span></p>
    <p class="c21"><span class="c7">Figure 1. Left: Meningioma - usually appear as an enhancing mass on the outside
            lining of the brain tissue, which may or may not brighten with contrast. Malignant meningiomas can also
            invade the brain tissue </span><span class="c3 c7"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/tr0z&amp;sa=D&amp;source=editors&amp;ust=1658367235318545&amp;usg=AOvVaw1rJtlZKd_EhaQYdjMwmuHT">[11]</a></span><span
            class="c7">. Middle: Glioma - called intra-axial brain tumors because they grow within the substance of the
            brain and often mix with normal brain tissue </span><span class="c3 c7"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/fCAf&amp;sa=D&amp;source=editors&amp;ust=1658367235318692&amp;usg=AOvVaw1igsiCuIA1aD3rQWCt8Z_h">[12]</a></span><span
            class="c7">. Right: Pituitary tumor - and abnormal growth in the pituitary gland. The pituitary is a small
            gland in the brain. It is located behind the back of the nose </span><span class="c3 c7"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/jmlR&amp;sa=D&amp;source=editors&amp;ust=1658367235318844&amp;usg=AOvVaw2JNEG0n-qC-bhmSdrIqzrD">[13]</a></span><span
            class="c0">. </span></p>
    <p class="c38"><span>&nbsp; &nbsp; </span><span>We used the prelabeled data as our source of truth for training the
            CNN and transfer learning models. We used precision, recall and F1 score to evaluate our models.</span></p>
    <p class="c17"><span class="c28">4. &nbsp; &nbsp;Methodology</span></p>
    <p class="c31"><span class="c2">&nbsp; &nbsp;In this project, we applied three deep learning techniques such as
            UNet, CNN and transfer learning for the classification task. We evaluated the UNet relative to another
            saliency map approach. We also compared the performance accuracy among the classification approaches. The
            applied methods are described as follows:</span></p>
    <p class="c32"><span class="c30 c10">4.1 &nbsp; &nbsp;Saliency Maps</span></p>
    <p class="c44"><span class="c2">&nbsp; &nbsp; To give brain tumor diagnosticians a better tool for diagnosing a type
            of brain tumor, it is not enough to predict the type of tumor present based on a contrast-enhanced MRI
            image, it&#39;s also important to also show the diagnostician what the model was looking at when it made the
            classification that it did. This is where saliency maps come in. A saliency map will show which neurons
            fired from the original image contributed most to the resulting classification. There are multiple methods
            to create saliency maps, we attempted a novel custom method and a known gradient tape method.</span></p>
    <p class="c32"><span class="c16 c10">4.1.1 &nbsp; &nbsp;UNet</span></p>
    <p class="c44"><span>&nbsp; &nbsp; </span><span class="c2">Our custom attempt at creating a saliency map involved
            using a UNet model. The following is the proposed technique: First, we train a UNet model to output the
            original input image. This first step&rsquo;s purpose is to create a decoder from a learned latent space.
            Our UNet structure is shown in figure 2. After training a full UNet, we train the encoder sub-structure of
            the UNet as shown in figure 3 separately as a classifier. To make the encoder a classifier, we extend the
            encoder with a flatten layer and a fully connected layer to train this sub-structure as a CNN classifier of
            the 3 brain tumor types. This step is so that the new latent space created by the CNN a.k.a encoder is tuned
            on the features that help classify the image. Third, we connect the newly trained encoder of the
            classification task with the previously trained decoder, as shown in figure 4. This final model will
            recreate the images from the latent space the classifier learned. Thus, our CNN model will classify the
            image and our decoder will tell us what the model was looking at when making this classification.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 273.60px; height: 588.01px;"><img
                alt="" src="images/image11.jpg"
                style="width: 273.60px; height: 588.01px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c20"><span class="c0">Figure 2: The UNet model used to train the decoder. Each down layer consists of a 2D
            convolution with 3x3 kernels and a batch normalization. Each up layer consists of 2D deconvolution with 3x3
            kernels and a batch normalization.</span></p>
    <p class="c14"><span>&nbsp; &nbsp; </span><span>To ensure the UNet model recreated the image from the latent space,
            we split the model into two side, a left side consisting of all the convolutional down sampling and up
            sampling layers and the right side consisting of one untrainable convolutional layer with a kernel of size
            1x1 and 1 feature space. The left layer is the full autoencoder which we are training. A known issue with
            using autoencoders when trying to regenerate images is that localization information tends to be lost in the
            encoding phase. Therefore a skip connection (the right layers), is used to pass the full image back through
            to the output.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 238.08px; height: 556.44px;"><img
                alt="" src="images/image6.jpg"
                style="width: 238.08px; height: 556.44px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c14"><span class="c0">Figure 3: The CNN/ encoder model. The name of the down layers are now class_down to
            be used later as the layers of the Final UNet&rsquo;s encoder. An extra dense layer is added for the
            classification task.</span></p>
    <p class="c14"><span>&nbsp; &nbsp; </span><span class="c2">Since the right layers pass the whole image to the output
            layers, the model can rely on only these layers to recreate the image. To prevent the model from relying on
            the right layers to recreate the image, we make the right layers convert from the 3 feature space consisting
            of red, green, and blue of the original image to a 1 feature space image using the 1x1 kernel convolutions.
            We further make all the layers on the right untrainable. In this configuration, </span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 302.40px; height: 633.55px;"><img
                alt="" src="images/image9.jpg"
                style="width: 302.40px; height: 633.55px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c14"><span class="c7">Figure 4: The Final UNet. The encoder layers are named class_down as they are taken
            from the classifier and the up layers are taken from the previously trained UNet. This UNet is not trained,
            just used to combine the pretrained weights and provide an image of what the encoder looks at.</span></p>
    <p class="c14"><span>all of the location details in the image are passed through the right layers without passing
            information about the color of each region. The coloring of the details are learned by the left layers. To
            prove this is the case, we look at the right (figure 6) and left (figure 5) layers separately as the model
            trains to confirm that the left layers are learning the correct coloring and the right layers are learning
            nothing.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 302.40px; height: 648.53px;"><img
                alt="" src="images/image14.jpg"
                style="width: 302.40px; height: 648.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c14"><span class="c7">Figure 5: This is the Left side only model. This model has a &lsquo;rand&rsquo;
            layer on the right instead of the &lsquo;location&rsquo; layer in the original UNet. This is so when the
            weights are loaded into this model, new weights are created in that layer. This is so we only use the
            weights trained on the right sub-layers only. This is to see the contribution of the left layers.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 302.40px; height: 157.25px;"><img
                alt="" src="images/image20.jpg"
                style="width: 302.40px; height: 157.25px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c14"><span class="c7">Figure 6: This is the right side of the original UNet model. The weights from the
            location layer are transferred from the trained UNet and the other weights are new random weights. This is
            to see the contribution of the location weights and prove they don&rsquo;t contribute to the color of the
            output image.</span></p>
    <p class="c32"><span class="c10 c16">4.1.2 &nbsp; &nbsp;Gradient Tape</span></p>
    <p class="c14"><span>&nbsp; &nbsp; </span><span class="c2">A common method for creating a saliency map for a CNN
            classifier is by using Tensorflow&rsquo;s built-in gradient tape functionality. The gradient tape saves the
            operations and gradients done through the network during a classification task. This gradient tape
            functionality is mainly used for the autograd functionality but can be hijacked to create saliency
            maps.</span></p>
    <p class="c44"><span class="c2">&nbsp; To create a saliency map using gradient tape, we can look at the gradients
            between the highest activated neuron, a.k.a the classification, in the last layer and the original image.
            This gradient represents which neurons from the original image contributed the most to the chosen neuron. We
            can then plot the neurons that had the most contribution in image shape to see which area in the original
            image the model was looking at when it decided on a classification. The results for this are shown in
            figures 13 and 14.</span></p>
    <p class="c32"><span class="c30 c10">4.2 &nbsp; &nbsp;Convolutional Neural Network</span></p>
    <p class="c32"><span class="c30 c10">4.2.1 &nbsp;CNN Structure and Parameter Settings</span></p>
    <p class="c32"><span class="c34">&nbsp; &nbsp;</span><span class="c2">The CNN model in this case is of the
            Sequential type. In Keras, the simplest technique to build a model is sequential. It allows one to
            layer-by-layer construct a model. The &lsquo;add()&rsquo; function is used to add layers to our model. For
            this CNN we have applied six layers of Conv2D layers which are convolution layers that will deal with the
            input images. 64 in the first layer, 256 in the second layer, 512 in the 3rd layer and 1024 in the last four
            layers are the number of nodes in each layer. This number is adjustable to be higher or lower, depending on
            the size of the given dataset. For CNN, this combination works well, so we stuck with this sequence. We have
            used ReLU as our activation function for the first six layers. In neural networks, this activation function
            performs extremely well. Our first layer takes in an input shape of image size 512,512,3 where 3 signifies
            that the images are RGB. We have added a &#39;Flatten&#39; layer between the last Conv2D layer and the Dense
            layer, which serves as a connection between the convolution and dense layers. We have used a Dense layer for
            our output layer as well. Dense is a standard layer type that is used in many cases for neural networks. We
            have 3 nodes in our output layer, one for each outcome (0&ndash;2). We have used activation
            &#39;Softmax&#39; for the output layer. After initializing the CNN, we used the maxpooling2D to down sample
            the images and lower the size of the feature maps by reducing the number of parameters to learn and the
            computation in the neural network. We have used a L2 regularizer of 0.001 in the first Dense layer in order
            to minimize the adjusted loss function and prevent overfitting or underfitting and a dropout of 0.1 after
            the first dense layer which is a regularization technique to prevent overfitting in the model.</span></p>
    <p class="c32"><span class="c30 c10">4.2.1 &nbsp; Compile</span></p>
    <p class="c32"><span class="c2">&nbsp; &nbsp;To compile the CNN we passed 3 parameters: optimizer, loss and metrics.
            We have used &lsquo;adam&rsquo; as our optimizer which adjusts the learning rate throughout training. We
            have used &lsquo;SparseCategoricalCrossentropy&rsquo; for our loss function. A lower loss score indicates
            that the model is performing better. We have used the &lsquo;accuracy&rsquo; metric to see the accuracy
            score on the validation set while training the model.</span></p>
    <p class="c32"><span class="c30 c10">4.3 &nbsp; &nbsp;Transfer Learning</span></p>
    <p class="c41"><span>&nbsp; &nbsp; </span><span>Transfer learning is a method where knowledge learnt from one task
            (source task) is transferred to learn a new task (target task) </span><span class="c3"><a class="c5"
                href="https://www.google.com/url?q=https://paperpile.com/c/qs0cg0/lXQF&amp;sa=D&amp;source=editors&amp;ust=1658367235321377&amp;usg=AOvVaw3mzbv0nPDp0R31uB2Bnkxu">[14]</a></span><span
            class="c2">. This approach can save time in modeling and improve the performance accuracy with less labeled
            data. In computer vision, several pretrained models such as VGG19, Inceptionv3, ResNet50 are available to
            leverage the knowledge transfer technique from one domain to another. </span></p>
    <p class="c46"><span>&nbsp; &nbsp; In this project, we applied transfer learning with the pretrained VGG16 model. We
            included one fully connected layer of 128 units followed by the classification layer with 3 units at the end
            of the model architecture. For this, we imported the VGG16 pretrained model and the necessary libraries
            using TensorFlow and Keras and all the implementation has been done in Python. The input image size is
            224x224 and batch size is 32. Moreover, to optimize the learning, we used &lsquo;Adam&rsquo; optimizer with
            0.001 learning rate. We chose these hyperparameters by trying different combinations of values. However, to
            prevent the overfitting we applied the early stopping approach.</span></p>
    <p class="c17"><span class="c37 c10">5. &nbsp; &nbsp;Results and Analysis</span></p>
    <p class="c17"><span class="c30 c10">5.1 &nbsp; &nbsp;Saliency Maps</span></p>
    <p class="c14"><span>&nbsp; &nbsp; We trained the UNet for 30 epochs and saved a UNet model every 5 epochs to later
            analyze. In figure 7, we can see the UNet&rsquo;s training progress and in figure 8, we can see the result
            of each UNet, showing it was better at coloring the image as it trained, saturating at around 25-30
            epochs.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 330.07px; height: 235.34px;"><img
                alt="" src="images/image13.png"
                style="width: 369.77px; height: 246.98px; margin-left: -8.89px; margin-top: -11.64px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c20"><span class="c0">Figure 7: This is the Loss vs. Epoch graph of the UNet model.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 342.20px; height: 192.62px;"><img
                alt="" src="images/image5.png"
                style="width: 342.20px; height: 192.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c20"><span class="c7">Figure 8: This is the resulting images from the UNet model through several epochs.
            From top left to bottom right epochs: 5, 10, 15, 20, 25, 30.</span></p>
    <p class="c14"><span>&nbsp; &nbsp; </span><span class="c2">To confirm only the left side of the network was
            contributing to the training, we also looked at the output of individual sides of the network. Figure 9
            shows the results of the right network with random values in the layers of the left side. The results show
            that the right network did not contribute to the coloring of the image, and just passed localization
            information. The left side&rsquo;s results are shown in figure 10. In these results, the left autoencoder
            side got the weights from the previously trained UNet model and random weights for the layers on the right
            side. The results show that this side of the network learned the coloring of the image.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.00px; height: 174.67px;"><img
                alt="" src="images/image17.png"
                style="width: 312.00px; height: 174.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c20"><span class="c0">Figure 9: This is the resulting images from the right sublayers of the UNet model
            through several epochs. From top left to bottom right epochs: 5, 10, 15, 20, 25, 30.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.00px; height: 174.67px;"><img
                alt="" src="images/image8.png"
                style="width: 312.00px; height: 174.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c20"><span class="c0">Figure 10: This is the resulting images from the left sublayers of the UNet model
            through several epochs. From top left to bottom right epochs: 5, 10, 15, 20, 25, 30.</span></p>
    <p class="c14"><span>&nbsp; &nbsp; </span><span class="c2">The encoder was then trained as a CNN for 160 epochs. The
            80-100 epochs are shown in figure 11. On approximately the 20th epoch, the model was performing at an 80-90
            percent training accuracy, but the confusion matrix was not very good. We trained the model for the extra
            140 epochs to make sure the encoder is hyper-tuned on the details in the image that will classify each tumor
            type. Figure 12 shows the confusion matrices of different stages in CNN&rsquo;s training.</span></p>
    <p class="c44"><span>&nbsp; &nbsp; In figures 13 and 14, we compare the UNet and gradient tape saliency methods. We
            see that the UNet method shows a more general region of where the model is looking and the gradient tape
            method shows more varied and exaggerated regions of where the model was looking. </span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 283.20px; height: 220.60px;"><img
                alt="" src="images/image2.png"
                style="width: 283.20px; height: 220.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c20"><span class="c0">Figure 11: This is the CNN/encoder training accuracy form epoch 80-100.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 479.34px; height: 296.28px;"><img
                alt="" src="images/image12.png"
                style="width: 479.34px; height: 296.28px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c20"><span class="c7">Figure 12: The confusion matrices for the CNN/encoder at several
            epochs.</span><span>&nbsp; </span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 292.80px; height: 177.54px;"><img
                alt="" src="images/image1.png"
                style="width: 292.80px; height: 177.54px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c20"><span class="c7">Figure 13: From top left to bottom right: original image, original UNet output, UNet
            with encoder trained as classifier, gradient tape saliency map, gradient tape saliency map on top of
            original image.</span></p>
    <p class="c38"><span class="c2">The gradient tape method also shows bright lines indicating the CNN was looking for
            vertical lines heavily during the classification task. The UNet method has a main brighter rectangular area
            around the whole head indicating the classification model was generally looking at the head, but more detail
            is not visible.</span></p>
    <p class="c20"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.00px; height: 181.33px;"><img
                alt="" src="images/image19.png"
                style="width: 312.00px; height: 181.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c50"><span class="c7">Figure 14: From top left to bottom right: original image, original UNet output, UNet
            with encoder trained as classifier, gradient tape saliency map, gradient tape saliency map on top of
            original image.</span></p>
    <p class="c17"><span class="c30 c10">5.2 &nbsp; &nbsp;Convolutional Neural Network</span></p>
    <p class="c17"><span class="c16 c10">5.2.1 &nbsp;Epochs and Accuracy and Loss Graphs</span></p>
    <p class="c17"><span>&nbsp; &nbsp; </span><span>We have run the CNN for 20 epochs and we got a training accuracy of
            &nbsp;0.98 and training loss of 0.47. We have achieved a Val accuracy of 0.93 and a val loss of 0.67.</span>
    </p>
    <p class="c8"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.00px; height: 142.67px;"><img
                alt="" src="images/image7.png"
                style="width: 312.00px; height: 142.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c8"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.00px; height: 146.67px;"><img
                alt="" src="images/image15.png"
                style="width: 312.00px; height: 146.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c8"><span class="c2">Figure 15. Accuracy and loss vs epochs</span></p>
    <p class="c17"><span class="c2">&nbsp; &nbsp; From the graph we can see that the loss and accuracy for both training
            and testing were almost consistent. We got a test accuracy of 92% and test loss of &nbsp;0.74 over 32 test
            samples.</span></p>
    <p class="c17"><span class="c16 c10">5.2.2 &nbsp;Confusion Matrix</span></p>
    <p class="c8"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.00px; height: 210.70px;"><img
                alt="" src="images/image18.png"
                style="width: 312.00px; height: 215.00px; margin-left: 0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c8"><span class="c0">Figure 16. Confusion matrix</span></p>
    <p class="c12"><span class="c16 c10"></span></p>
    <p class="c12"><span class="c16 c10"></span></p>
    <p class="c12"><span class="c16 c10"></span></p>
    <p class="c12"><span class="c16 c10"></span></p>
    <p class="c12"><span class="c16 c10"></span></p>
    <p class="c12"><span class="c16 c10"></span></p>
    <p class="c17"><span class="c40">5.2.3 &nbsp;Precision, Recall and F1-Score per Class Using CNN</span></p><a
        id="t.5dd012539be15a503882e49f4d065ecad86c17ad"></a><a id="t.0"></a>
    <table class="c51">
        <tr class="c22">
            <td class="c29" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">Class</span></p>
            </td>
            <td class="c36" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">Precision</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">Recall</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">F1-score</span></p>
            </td>
        </tr>
        <tr class="c22">
            <td class="c29" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">Glioma</span></p>
            </td>
            <td class="c36" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">93%</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">93%</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">93%</span></p>
            </td>
        </tr>
        <tr class="c22">
            <td class="c29" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">Meningioma</span></p>
            </td>
            <td class="c36" colspan="1" rowspan="1">
                <p class="c1"><span class="c3 c4">86%</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">86%</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">86%</span></p>
            </td>
        </tr>
        <tr class="c22">
            <td class="c29" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">Pituitary tumor</span></p>
            </td>
            <td class="c36" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">100%</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">100%</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">100%</span></p>
            </td>
        </tr>
        <tr class="c22">
            <td class="c29" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">Average</span></p>
            </td>
            <td class="c36" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">93%</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">93%</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">93%</span></p>
            </td>
        </tr>
    </table>
    <p class="c20"><span class="c7">Table 1.</span><span class="c7 c40">&nbsp;</span><span class="c7">Precision, recall
            and F1-score for each class using custom CNN.</span></p>
    <p class="c12"><span class="c30 c10"></span></p>
    <p class="c17"><span class="c30 c10">5.3 &nbsp; &nbsp;Transfer Learning</span></p>
    <p class="c14"><span>&nbsp; &nbsp; We divided the dataset into training, validation and testing dataset. The
            training, validation and testing set contain 2452, 420 and 192 images respectively. Transfer learning has
            the power to show better accuracy in very early epochs and so, the model showed preferable accuracy and was
            trained for only 9 epochs. After training, the model was evaluated on the test set and the approach got
            93.23% accuracy. We have also provided the confusion matrix (Figure 17) and the precision, recall and F1
            scores in Table 1 to quantify the recognition of the trained classifier for each of the 3
            classes.</span><span class="c33">&nbsp;</span></p>
    <p class="c14 c27"><span class="c0"></span></p><a id="t.e682dd97b7a6e534355eda6ecdb1ac1a08d808cd"></a><a
        id="t.1"></a>
    <table class="c51">
        <tr class="c22">
            <td class="c15" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">Class</span></p>
            </td>
            <td class="c25" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">Precision</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">Recall</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">F1-score</span></p>
            </td>
        </tr>
        <tr class="c22">
            <td class="c15" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">Glioma</span></p>
            </td>
            <td class="c25" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">95%</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">91%</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">93%</span></p>
            </td>
        </tr>
        <tr class="c22">
            <td class="c15" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">Meningioma</span></p>
            </td>
            <td class="c25" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">78%</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">84%</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">81%</span></p>
            </td>
        </tr>
        <tr class="c22">
            <td class="c15" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">Pituitary tumor</span></p>
            </td>
            <td class="c25" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">95%</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">97%</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c3">96%</span></p>
            </td>
        </tr>
        <tr class="c22">
            <td class="c15" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">Average</span></p>
            </td>
            <td class="c25" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">90%</span></p>
            </td>
            <td class="c9" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">91%</span></p>
            </td>
            <td class="c6" colspan="1" rowspan="1">
                <p class="c1"><span class="c4 c10">90%</span></p>
            </td>
        </tr>
    </table>
    <p class="c20"><span class="c7">Table 2.</span><span class="c40 c7">&nbsp;</span><span class="c0">Precision, recall
            and F1-score for each class using transfer learning</span></p>
    <p class="c35"><span
            style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 229.57px; height: 240.05px;"><img
                alt="" src="images/image21.png"
                style="width: 229.57px; height: 248.43px; margin-left: 0.00px; margin-top: -8.38px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                title=""></span></p>
    <p class="c35"><span class="c7">Figure 17. Confusion matrix.</span></p>
    <p class="c17"><span class="c28">6. &nbsp; &nbsp;Conclusion and Future Work</span></p>
    <p class="c14"><span>&nbsp; &nbsp; Brain tumor classification is one of the challenging tasks according to
            clinicians as the misclassification can occur due to bias and errors. In this project, we implemented three
            techniques in classifying three types of brain tumor. We also created several saliency maps using a UNet
            method and a gradient tape method.</span><span>&nbsp;</span><span class="c2">Our saliency maps provided
            insight into what the CNN model was learning but did not prove an adequate tool to localize the tumors in
            the image. Our custom CNN and transfer learning approach acquired 92% and 93.23% accuracy on the test set
            respectively. Our approaches provided an acceptable performance in the classification task and can assist
            the radiologists to categorize the brain tumor from the MRI.</span></p>
    <p class="c44"><span class="c2">&nbsp; &nbsp; Neither the UNet method nor the gradient tape method provided much
            insight into where the tumor might be in the image. The UNet method provided a generalized area of where it
            was looking in the image without specifying very deeply what subregions it was looking at. The gradient tape
            method provided very specific and varied areas in the image. These regions did not help specify where the
            brain tumor could be. </span></p>
    <p class="c44"><span class="c2">&nbsp; &nbsp; Although there are similar ideas to it, we have not found our UNet
            method in other literature. It might not have performed very well at localizing the brain tumor in our data,
            but it did show the general area the model was focusing on. This means that with images where the more
            obvious information in the image leads to the classification, the UNet method would be able to show which
            region the model looked at. Thus, the model &nbsp;has potential as a region proposal method trained on only
            the classifications of an image. This model can generalize to give regions that affected the classification
            and propose where the objects in the image are. The main benefit for this method as a region proposal method
            is that the regions of the data do not need to be labeled ahead of time. Furthermore, the regions proposed
            by the UNet will not be restricted to specific shapes and can be much more flexible. </span></p>
    <p class="c44"><span class="c2">&nbsp; &nbsp; The UNet method can also be used as a label creation technique for
            another object detection model to train on. Instead of having a person label the location of objects in an
            image. The UNet can be trained with just the classification of the object and it will generate the location
            that was affecting this classification, thus creating some general localization regions which could be used
            as labels for an object detection model.</span></p>
    <p class="c44"><span class="c2">&nbsp; &nbsp; To improve the UNet model, we could also introduce more dropout layers
            to force brighter smaller regions. The decoder layers might be too well tuned at recreating the original
            image and in a sense ignore the differences in the latent space from the new classification information. To
            improve on this, we would need to make the model more sensitive to the latent space layer.</span></p>
    <h1 class="c23 c52"><span class="c28">References</span></h1>
    <p class="c43"><span class="c0">1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/OBP1&amp;sa=D&amp;source=editors&amp;ust=1658367235333367&amp;usg=AOvVaw2FEFfer0hChHPpSHIg7ism">Dougherty
                G. Digital Image Processing for Medical Applications. 2009. doi:</a></span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://dx.doi.org/10.1017/cbo9780511609657&amp;sa=D&amp;source=editors&amp;ust=1658367235333697&amp;usg=AOvVaw2n3c_O16uX43kUhU4CBza7">10.1017/cbo9780511609657</a></span>
    </p>
    <p class="c18"><span class="c0">2. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/tGM1&amp;sa=D&amp;source=editors&amp;ust=1658367235333930&amp;usg=AOvVaw3SjHQdKCn7n_JPQmerx6iZ">de
                Bruijne M. Machine learning approaches in medical image analysis: From detection to diagnosis. Med Image
                Anal. 2016;33: 94&ndash;97.</a></span></p>
    <p class="c18"><span class="c0">3. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/WN5aI&amp;sa=D&amp;source=editors&amp;ust=1658367235334120&amp;usg=AOvVaw2mFkp3NuqIDi5UVLTvE5lc">Khan
                AH, Abbas S, Khan MA, Farooq U, Khan WA, Siddiqui SY, et al. Intelligent Model for Brain Tumor
                Identification Using Deep Learning. Applied Computational Intelligence and Soft Computing. 2022;2022.
                doi:</a></span><span class="c0"><a class="c5"
                href="https://www.google.com/url?q=http://dx.doi.org/10.1155/2022/8104054&amp;sa=D&amp;source=editors&amp;ust=1658367235334289&amp;usg=AOvVaw3RyT2GgpXoZ17YHLKhuElH">10.1155/2022/8104054</a></span>
    </p>
    <p class="c18"><span class="c0">4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/9d8n&amp;sa=D&amp;source=editors&amp;ust=1658367235334549&amp;usg=AOvVaw1tePOnEQacJCTVuZ2DSdQu">Mantha
                T, Eswara Reddy B. A Transfer Learning method for Brain Tumor Classification using EfficientNet-B3
                model. 2021 IEEE International Conference on Computation System and Information Technology for
                Sustainable Solutions (CSITSS). 2021. pp. 1&ndash;6.</a></span></p>
    <p class="c18"><span class="c0">5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/zy2f&amp;sa=D&amp;source=editors&amp;ust=1658367235334748&amp;usg=AOvVaw0QKNb0prwvv0IT2ii_ufiF">D&iacute;az-Pernas
                FJ, Mart&iacute;nez-Zarzuela M, Ant&oacute;n-Rodr&iacute;guez M, Gonz&aacute;lez-Ortega D. A Deep
                Learning Approach for Brain Tumor Classification and Segmentation Using a Multiscale Convolutional
                Neural Network. Healthcare (Basel). 2021;9. doi:</a></span><span class="c0"><a class="c5"
                href="https://www.google.com/url?q=http://dx.doi.org/10.3390/healthcare9020153&amp;sa=D&amp;source=editors&amp;ust=1658367235334903&amp;usg=AOvVaw08YKmL5U6za5VARXJcB5AL">10.3390/healthcare9020153</a></span>
    </p>
    <p class="c18"><span class="c0">6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/Mmgv&amp;sa=D&amp;source=editors&amp;ust=1658367235335139&amp;usg=AOvVaw2hWHBwt1q5X7a4pRrzcj9x">Irmak
                E. Multi-Classification of Brain Tumor MRI Images Using Deep Convolutional Neural Network with Fully
                Optimized Framework. Iranian Journal of Science and Technology, Transactions of Electrical Engineering.
                2021;45: 1015&ndash;1036.</a></span></p>
    <p class="c18"><span class="c0">7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/SSvs&amp;sa=D&amp;source=editors&amp;ust=1658367235335344&amp;usg=AOvVaw2_f3C59n1bE2U_nYeXByBu">Soumik
                MFI, Hossain MA. Brain Tumor Classification With Inception Network Based Deep Learning Model Using
                Transfer Learning. 2020 IEEE Region 10 Symposium (TENSYMP). 2020. pp. 1018&ndash;1021.</a></span></p>
    <p class="c18"><span class="c0">8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/IsN7&amp;sa=D&amp;source=editors&amp;ust=1658367235335547&amp;usg=AOvVaw06x5hsEarwl13Z5TNd5EOv">Cheng
                J, Huang W, Cao S, Yang R, Yang W, Yun Z, et al. Enhanced Performance of Brain Tumor Classification via
                Tumor Region Augmentation and Partition. PLOS ONE. 2015. p. e0140381. doi:</a></span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://dx.doi.org/10.1371/journal.pone.0140381&amp;sa=D&amp;source=editors&amp;ust=1658367235335705&amp;usg=AOvVaw2LI6oNHkhbDiWQSABLc0EI">10.1371/journal.pone.0140381</a></span>
    </p>
    <p class="c18"><span class="c0">9. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/lTF7&amp;sa=D&amp;source=editors&amp;ust=1658367235335900&amp;usg=AOvVaw2Nhk3SYcy3ubmDwBpLB6T7">Cheng
                J, Yang W, Huang M, Huang W, Jiang J, Zhou Y, et al. Retrieval of Brain Tumors by Adaptive Spatial
                Pooling and Fisher Vector Representation. PLoS One. 2016;11: e0157112.</a></span></p>
    <p class="c18"><span class="c0">10. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/nxk4&amp;sa=D&amp;source=editors&amp;ust=1658367235336114&amp;usg=AOvVaw2-q74dhx4AiEGLf-WzLiyP">Kavi
                D. Brain Tumor Image Dataset. Available: </a></span><span class="c0"><a class="c5"
                href="https://www.google.com/url?q=https://www.kaggle.com/denizkavi1/brain-tumor&amp;sa=D&amp;source=editors&amp;ust=1658367235336248&amp;usg=AOvVaw1oTFf0PcgR9v1EDmafUuah">https://www.kaggle.com/denizkavi1/brain-tumor</a></span>
    </p>
    <p class="c18"><span class="c0">11. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/tr0z&amp;sa=D&amp;source=editors&amp;ust=1658367235336447&amp;usg=AOvVaw1k80R27kapHEMDjsOK8X2t">Meningioma
                diagnosis and treatment. In: National Cancer Institute [Internet]. 17 Sep 2018 [cited 23 Feb 2022].
                Available: </a></span><span class="c0"><a class="c5"
                href="https://www.google.com/url?q=https://www.cancer.gov/rare-brain-spine-tumor/tumors/meningioma&amp;sa=D&amp;source=editors&amp;ust=1658367235336601&amp;usg=AOvVaw2YyMx_mShouHIolukhKhvb">https://www.cancer.gov/rare-brain-spine-tumor/tumors/meningioma</a></span>
    </p>
    <p class="c18"><span class="c0">12. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/fCAf&amp;sa=D&amp;source=editors&amp;ust=1658367235336818&amp;usg=AOvVaw3dpMHMvnXZB3d4jCyMNPjC">Gliomas.
                [cited 23 Feb 2022]. Available: </a></span><span class="c0"><a class="c5"
                href="https://www.google.com/url?q=https://www.hopkinsmedicine.org/health/conditions-and-diseases/gliomas&amp;sa=D&amp;source=editors&amp;ust=1658367235337022&amp;usg=AOvVaw3J-YcRB04E7euz8ymxO-Qp">https://www.hopkinsmedicine.org/health/conditions-and-diseases/gliomas</a></span>
    </p>
    <p class="c18"><span class="c0">13. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/jmlR&amp;sa=D&amp;source=editors&amp;ust=1658367235337235&amp;usg=AOvVaw3kJGeMdbA40vqd6X4Q0-MV">Pituitary
                tumors. [cited 23 Feb 2022]. Available: </a></span><span class="c0"><a class="c5"
                href="https://www.google.com/url?q=https://www.hopkinsmedicine.org/health/conditions-and-diseases/pituitary-tumors&amp;sa=D&amp;source=editors&amp;ust=1658367235337397&amp;usg=AOvVaw0dw-zEMnKF1q_EPHHuzrYo">https://www.hopkinsmedicine.org/health/conditions-and-diseases/pituitary-tumors</a></span>
    </p>
    <p class="c18"><span class="c0">14. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a
                class="c5"
                href="https://www.google.com/url?q=http://paperpile.com/b/qs0cg0/lXQF&amp;sa=D&amp;source=editors&amp;ust=1658367235337597&amp;usg=AOvVaw0ShqIVg4S0H0-ezQWyEyEn">Pan
                SJ, Yang Q. A Survey on Transfer Learning. IEEE Trans Knowl Data Eng. 2010;22:
                1345&ndash;1359.</a></span></p>
    <p class="c27 c63"><span class="c0"></span></p>
    <div>
        <p class="c23 c27 c39"><span class="c2"></span></p>
    </div>
</body>

</html>