<html>

<head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <style type="text/css">
        @import url('https://themes.googleusercontent.com/fonts/css?kit=fAgBujFnYvD1u9rhbIlZ1B06if6osnyAslCuLPPf50A');

        .lst-kix_sfxyl3uwwh5k-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_sfxyl3uwwh5k-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_jklb5xavpn79-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_2odvspw48wt1-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_2odvspw48wt1-8>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_7gynq2rc690l-1 {
            list-style-type: none
        }

        .lst-kix_sfxyl3uwwh5k-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_sfxyl3uwwh5k-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_jklb5xavpn79-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_jklb5xavpn79-7>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_7gynq2rc690l-2 {
            list-style-type: none
        }

        .lst-kix_2odvspw48wt1-7>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_7gynq2rc690l-0 {
            list-style-type: none
        }

        ul.lst-kix_7gynq2rc690l-5 {
            list-style-type: none
        }

        ul.lst-kix_7gynq2rc690l-6 {
            list-style-type: none
        }

        ul.lst-kix_7gynq2rc690l-3 {
            list-style-type: none
        }

        .lst-kix_2odvspw48wt1-4>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_7gynq2rc690l-4 {
            list-style-type: none
        }

        .lst-kix_sfxyl3uwwh5k-3>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_7gynq2rc690l-7 {
            list-style-type: none
        }

        .lst-kix_2odvspw48wt1-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_ba0geepg59w6-0>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_7gynq2rc690l-8 {
            list-style-type: none
        }

        .lst-kix_2odvspw48wt1-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_jklb5xavpn79-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_ba0geepg59w6-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_2odvspw48wt1-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_ba0geepg59w6-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_ba0geepg59w6-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_ba0geepg59w6-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_2odvspw48wt1-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_2odvspw48wt1-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_ba0geepg59w6-4>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_j3gz6m1fbjdi-8 {
            list-style-type: none
        }

        .lst-kix_ba0geepg59w6-7>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_j3gz6m1fbjdi-4 {
            list-style-type: none
        }

        ul.lst-kix_j3gz6m1fbjdi-5 {
            list-style-type: none
        }

        ul.lst-kix_j3gz6m1fbjdi-6 {
            list-style-type: none
        }

        ul.lst-kix_j3gz6m1fbjdi-7 {
            list-style-type: none
        }

        .lst-kix_ba0geepg59w6-6>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_j3gz6m1fbjdi-0 {
            list-style-type: none
        }

        ul.lst-kix_j3gz6m1fbjdi-1 {
            list-style-type: none
        }

        ul.lst-kix_j3gz6m1fbjdi-2 {
            list-style-type: none
        }

        .lst-kix_of2uv8ooex53-2>li {
            counter-increment: lst-ctn-kix_of2uv8ooex53-2
        }

        ul.lst-kix_j3gz6m1fbjdi-3 {
            list-style-type: none
        }

        .lst-kix_ba0geepg59w6-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_jklb5xavpn79-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_sfxyl3uwwh5k-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_jklb5xavpn79-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_jklb5xavpn79-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_sfxyl3uwwh5k-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_sfxyl3uwwh5k-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_jklb5xavpn79-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_jklb5xavpn79-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_sfxyl3uwwh5k-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_j3gz6m1fbjdi-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_j3gz6m1fbjdi-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_j3gz6m1fbjdi-8>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_8i06obw4pk94-8 {
            list-style-type: none
        }

        .lst-kix_nnit3y3vmwmk-8>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_8i06obw4pk94-6 {
            list-style-type: none
        }

        ul.lst-kix_8i06obw4pk94-7 {
            list-style-type: none
        }

        ul.lst-kix_8i06obw4pk94-4 {
            list-style-type: none
        }

        ul.lst-kix_8i06obw4pk94-5 {
            list-style-type: none
        }

        .lst-kix_j3gz6m1fbjdi-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_j3gz6m1fbjdi-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_j3gz6m1fbjdi-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_nnit3y3vmwmk-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_of2uv8ooex53-6>li {
            counter-increment: lst-ctn-kix_of2uv8ooex53-6
        }

        .lst-kix_nnit3y3vmwmk-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_nnit3y3vmwmk-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_nnit3y3vmwmk-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_nnit3y3vmwmk-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_nnit3y3vmwmk-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_nnit3y3vmwmk-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_nnit3y3vmwmk-1>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_8i06obw4pk94-2 {
            list-style-type: none
        }

        ul.lst-kix_8i06obw4pk94-3 {
            list-style-type: none
        }

        ul.lst-kix_8i06obw4pk94-0 {
            list-style-type: none
        }

        ul.lst-kix_8i06obw4pk94-1 {
            list-style-type: none
        }

        .lst-kix_hmrk9a74a5bc-2>li:before {
            content: "\0025a0  "
        }

        ol.lst-kix_of2uv8ooex53-8.start {
            counter-reset: lst-ctn-kix_of2uv8ooex53-8 0
        }

        .lst-kix_hmrk9a74a5bc-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_hmrk9a74a5bc-8>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_nnit3y3vmwmk-2 {
            list-style-type: none
        }

        ul.lst-kix_nnit3y3vmwmk-1 {
            list-style-type: none
        }

        ul.lst-kix_nnit3y3vmwmk-4 {
            list-style-type: none
        }

        ul.lst-kix_nnit3y3vmwmk-3 {
            list-style-type: none
        }

        ul.lst-kix_nnit3y3vmwmk-0 {
            list-style-type: none
        }

        .lst-kix_hmrk9a74a5bc-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_hmrk9a74a5bc-4>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_ex1fp6bfjdoc-0 {
            list-style-type: none
        }

        .lst-kix_jx3xljry6r5h-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_jx3xljry6r5h-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_jx3xljry6r5h-5>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_nnit3y3vmwmk-6 {
            list-style-type: none
        }

        ul.lst-kix_nnit3y3vmwmk-5 {
            list-style-type: none
        }

        ul.lst-kix_nnit3y3vmwmk-8 {
            list-style-type: none
        }

        ul.lst-kix_nnit3y3vmwmk-7 {
            list-style-type: none
        }

        .lst-kix_jx3xljry6r5h-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_j3gz6m1fbjdi-1>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_jklb5xavpn79-2 {
            list-style-type: none
        }

        ul.lst-kix_jklb5xavpn79-3 {
            list-style-type: none
        }

        ul.lst-kix_jklb5xavpn79-4 {
            list-style-type: none
        }

        ul.lst-kix_jklb5xavpn79-5 {
            list-style-type: none
        }

        ul.lst-kix_jklb5xavpn79-6 {
            list-style-type: none
        }

        ul.lst-kix_jklb5xavpn79-7 {
            list-style-type: none
        }

        ul.lst-kix_jklb5xavpn79-8 {
            list-style-type: none
        }

        ul.lst-kix_98g8k6iqnvhs-7 {
            list-style-type: none
        }

        ul.lst-kix_98g8k6iqnvhs-8 {
            list-style-type: none
        }

        ul.lst-kix_98g8k6iqnvhs-5 {
            list-style-type: none
        }

        ul.lst-kix_98g8k6iqnvhs-6 {
            list-style-type: none
        }

        ul.lst-kix_jklb5xavpn79-0 {
            list-style-type: none
        }

        ul.lst-kix_98g8k6iqnvhs-3 {
            list-style-type: none
        }

        ul.lst-kix_jklb5xavpn79-1 {
            list-style-type: none
        }

        ul.lst-kix_98g8k6iqnvhs-4 {
            list-style-type: none
        }

        ul.lst-kix_98g8k6iqnvhs-1 {
            list-style-type: none
        }

        ul.lst-kix_98g8k6iqnvhs-2 {
            list-style-type: none
        }

        ul.lst-kix_98g8k6iqnvhs-0 {
            list-style-type: none
        }

        ul.lst-kix_ex1fp6bfjdoc-3 {
            list-style-type: none
        }

        ul.lst-kix_ex1fp6bfjdoc-4 {
            list-style-type: none
        }

        ul.lst-kix_ex1fp6bfjdoc-1 {
            list-style-type: none
        }

        ul.lst-kix_ex1fp6bfjdoc-2 {
            list-style-type: none
        }

        ul.lst-kix_ex1fp6bfjdoc-7 {
            list-style-type: none
        }

        ul.lst-kix_ex1fp6bfjdoc-8 {
            list-style-type: none
        }

        ul.lst-kix_ex1fp6bfjdoc-5 {
            list-style-type: none
        }

        ul.lst-kix_ex1fp6bfjdoc-6 {
            list-style-type: none
        }

        .lst-kix_h9ktri7fkm9w-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_h9ktri7fkm9w-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_rt08gvu9v8s5-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_h9ktri7fkm9w-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_rt08gvu9v8s5-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_rt08gvu9v8s5-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_8i06obw4pk94-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_8i06obw4pk94-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_h9ktri7fkm9w-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_rt08gvu9v8s5-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_8i06obw4pk94-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_h9ktri7fkm9w-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_of2uv8ooex53-3>li {
            counter-increment: lst-ctn-kix_of2uv8ooex53-3
        }

        .lst-kix_8i06obw4pk94-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_of2uv8ooex53-5>li {
            counter-increment: lst-ctn-kix_of2uv8ooex53-5
        }

        .lst-kix_8i06obw4pk94-0>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_fkpbq93k7p5m-0 {
            list-style-type: none
        }

        ul.lst-kix_fkpbq93k7p5m-5 {
            list-style-type: none
        }

        ul.lst-kix_fkpbq93k7p5m-6 {
            list-style-type: none
        }

        ul.lst-kix_fkpbq93k7p5m-7 {
            list-style-type: none
        }

        ul.lst-kix_fkpbq93k7p5m-8 {
            list-style-type: none
        }

        ul.lst-kix_fkpbq93k7p5m-1 {
            list-style-type: none
        }

        ul.lst-kix_fkpbq93k7p5m-2 {
            list-style-type: none
        }

        ul.lst-kix_fkpbq93k7p5m-3 {
            list-style-type: none
        }

        ul.lst-kix_fkpbq93k7p5m-4 {
            list-style-type: none
        }

        .lst-kix_68op9ugufxnr-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_68op9ugufxnr-6>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_jx3xljry6r5h-2 {
            list-style-type: none
        }

        ul.lst-kix_jx3xljry6r5h-3 {
            list-style-type: none
        }

        ul.lst-kix_jx3xljry6r5h-4 {
            list-style-type: none
        }

        ul.lst-kix_jx3xljry6r5h-5 {
            list-style-type: none
        }

        ul.lst-kix_jx3xljry6r5h-6 {
            list-style-type: none
        }

        ul.lst-kix_jx3xljry6r5h-7 {
            list-style-type: none
        }

        ul.lst-kix_jx3xljry6r5h-8 {
            list-style-type: none
        }

        .lst-kix_68op9ugufxnr-3>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_q6t6mv5orthl-0 {
            list-style-type: none
        }

        .lst-kix_68op9ugufxnr-2>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_jx3xljry6r5h-0 {
            list-style-type: none
        }

        ul.lst-kix_jx3xljry6r5h-1 {
            list-style-type: none
        }

        ul.lst-kix_q6t6mv5orthl-5 {
            list-style-type: none
        }

        ul.lst-kix_q6t6mv5orthl-6 {
            list-style-type: none
        }

        ul.lst-kix_q6t6mv5orthl-7 {
            list-style-type: none
        }

        ul.lst-kix_q6t6mv5orthl-8 {
            list-style-type: none
        }

        ul.lst-kix_q6t6mv5orthl-1 {
            list-style-type: none
        }

        ul.lst-kix_q6t6mv5orthl-2 {
            list-style-type: none
        }

        ul.lst-kix_q6t6mv5orthl-3 {
            list-style-type: none
        }

        ul.lst-kix_q6t6mv5orthl-4 {
            list-style-type: none
        }

        .lst-kix_hmrk9a74a5bc-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_fkpbq93k7p5m-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_hmrk9a74a5bc-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_jw84dwe01i4q-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_fkpbq93k7p5m-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_jx3xljry6r5h-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_98g8k6iqnvhs-5>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_ba0geepg59w6-0 {
            list-style-type: none
        }

        .lst-kix_klcvbh73so8p-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_jx3xljry6r5h-4>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_jw84dwe01i4q-3 {
            list-style-type: none
        }

        .lst-kix_jw84dwe01i4q-7>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_jw84dwe01i4q-4 {
            list-style-type: none
        }

        ul.lst-kix_jw84dwe01i4q-1 {
            list-style-type: none
        }

        ul.lst-kix_jw84dwe01i4q-2 {
            list-style-type: none
        }

        ul.lst-kix_jw84dwe01i4q-0 {
            list-style-type: none
        }

        .lst-kix_klcvbh73so8p-0>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_ba0geepg59w6-2 {
            list-style-type: none
        }

        ul.lst-kix_ba0geepg59w6-1 {
            list-style-type: none
        }

        .lst-kix_j3gz6m1fbjdi-2>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_ba0geepg59w6-4 {
            list-style-type: none
        }

        ul.lst-kix_ba0geepg59w6-3 {
            list-style-type: none
        }

        ul.lst-kix_ba0geepg59w6-6 {
            list-style-type: none
        }

        ul.lst-kix_ba0geepg59w6-5 {
            list-style-type: none
        }

        ul.lst-kix_ba0geepg59w6-8 {
            list-style-type: none
        }

        .lst-kix_jx3xljry6r5h-0>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_ba0geepg59w6-7 {
            list-style-type: none
        }

        .lst-kix_of2uv8ooex53-7>li:before {
            content: "" counter(lst-ctn-kix_of2uv8ooex53-7, lower-latin) ". "
        }

        .lst-kix_q6t6mv5orthl-4>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_jw84dwe01i4q-7 {
            list-style-type: none
        }

        ul.lst-kix_jw84dwe01i4q-8 {
            list-style-type: none
        }

        ul.lst-kix_jw84dwe01i4q-5 {
            list-style-type: none
        }

        ul.lst-kix_jw84dwe01i4q-6 {
            list-style-type: none
        }

        .lst-kix_98g8k6iqnvhs-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_q6t6mv5orthl-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_ex1fp6bfjdoc-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_klcvbh73so8p-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_7gynq2rc690l-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_ex1fp6bfjdoc-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_7gynq2rc690l-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_q6t6mv5orthl-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_of2uv8ooex53-4>li {
            counter-increment: lst-ctn-kix_of2uv8ooex53-4
        }

        ol.lst-kix_of2uv8ooex53-4.start {
            counter-reset: lst-ctn-kix_of2uv8ooex53-4 0
        }

        .lst-kix_of2uv8ooex53-3>li:before {
            content: "" counter(lst-ctn-kix_of2uv8ooex53-3, decimal) ". "
        }

        .lst-kix_of2uv8ooex53-4>li:before {
            content: "" counter(lst-ctn-kix_of2uv8ooex53-4, lower-latin) ". "
        }

        .lst-kix_of2uv8ooex53-6>li:before {
            content: "" counter(lst-ctn-kix_of2uv8ooex53-6, decimal) ". "
        }

        .lst-kix_of2uv8ooex53-5>li:before {
            content: "" counter(lst-ctn-kix_of2uv8ooex53-5, lower-roman) ". "
        }

        ul.lst-kix_rt08gvu9v8s5-3 {
            list-style-type: none
        }

        .lst-kix_xqchyxri23mz-3>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_rt08gvu9v8s5-2 {
            list-style-type: none
        }

        ul.lst-kix_rt08gvu9v8s5-1 {
            list-style-type: none
        }

        ul.lst-kix_rt08gvu9v8s5-0 {
            list-style-type: none
        }

        ul.lst-kix_rt08gvu9v8s5-7 {
            list-style-type: none
        }

        ul.lst-kix_rt08gvu9v8s5-6 {
            list-style-type: none
        }

        ul.lst-kix_rt08gvu9v8s5-5 {
            list-style-type: none
        }

        .lst-kix_xqchyxri23mz-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_xqchyxri23mz-6>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_rt08gvu9v8s5-4 {
            list-style-type: none
        }

        ul.lst-kix_rt08gvu9v8s5-8 {
            list-style-type: none
        }

        .lst-kix_of2uv8ooex53-2>li:before {
            content: "" counter(lst-ctn-kix_of2uv8ooex53-2, lower-roman) ". "
        }

        .lst-kix_xqchyxri23mz-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_of2uv8ooex53-1>li:before {
            content: "" counter(lst-ctn-kix_of2uv8ooex53-1, lower-latin) ". "
        }

        .lst-kix_xqchyxri23mz-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_of2uv8ooex53-0>li:before {
            content: "" counter(lst-ctn-kix_of2uv8ooex53-0, decimal) ". "
        }

        .lst-kix_of2uv8ooex53-0>li {
            counter-increment: lst-ctn-kix_of2uv8ooex53-0
        }

        .lst-kix_xqchyxri23mz-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_xqchyxri23mz-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_xqchyxri23mz-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_xqchyxri23mz-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_jw84dwe01i4q-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_jw84dwe01i4q-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_fkpbq93k7p5m-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_fkpbq93k7p5m-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_fkpbq93k7p5m-4>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_h9ktri7fkm9w-5 {
            list-style-type: none
        }

        ul.lst-kix_h9ktri7fkm9w-6 {
            list-style-type: none
        }

        .lst-kix_jw84dwe01i4q-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_fkpbq93k7p5m-0>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_h9ktri7fkm9w-7 {
            list-style-type: none
        }

        ul.lst-kix_h9ktri7fkm9w-8 {
            list-style-type: none
        }

        ul.lst-kix_h9ktri7fkm9w-1 {
            list-style-type: none
        }

        ul.lst-kix_h9ktri7fkm9w-2 {
            list-style-type: none
        }

        ul.lst-kix_h9ktri7fkm9w-3 {
            list-style-type: none
        }

        ul.lst-kix_h9ktri7fkm9w-4 {
            list-style-type: none
        }

        ul.lst-kix_sfxyl3uwwh5k-2 {
            list-style-type: none
        }

        ul.lst-kix_sfxyl3uwwh5k-1 {
            list-style-type: none
        }

        ul.lst-kix_sfxyl3uwwh5k-4 {
            list-style-type: none
        }

        ul.lst-kix_sfxyl3uwwh5k-3 {
            list-style-type: none
        }

        .lst-kix_klcvbh73so8p-5>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_h9ktri7fkm9w-0 {
            list-style-type: none
        }

        ul.lst-kix_sfxyl3uwwh5k-0 {
            list-style-type: none
        }

        .lst-kix_98g8k6iqnvhs-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_klcvbh73so8p-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_klcvbh73so8p-3>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_sfxyl3uwwh5k-6 {
            list-style-type: none
        }

        ul.lst-kix_sfxyl3uwwh5k-5 {
            list-style-type: none
        }

        ul.lst-kix_sfxyl3uwwh5k-8 {
            list-style-type: none
        }

        ul.lst-kix_sfxyl3uwwh5k-7 {
            list-style-type: none
        }

        .lst-kix_98g8k6iqnvhs-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_jw84dwe01i4q-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_jw84dwe01i4q-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_of2uv8ooex53-8>li {
            counter-increment: lst-ctn-kix_of2uv8ooex53-8
        }

        .lst-kix_fkpbq93k7p5m-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_98g8k6iqnvhs-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_of2uv8ooex53-1>li {
            counter-increment: lst-ctn-kix_of2uv8ooex53-1
        }

        .lst-kix_of2uv8ooex53-8>li:before {
            content: "" counter(lst-ctn-kix_of2uv8ooex53-8, lower-roman) ". "
        }

        .lst-kix_q6t6mv5orthl-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_of2uv8ooex53-7>li {
            counter-increment: lst-ctn-kix_of2uv8ooex53-7
        }

        .lst-kix_q6t6mv5orthl-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_7gynq2rc690l-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_q6t6mv5orthl-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_98g8k6iqnvhs-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_98g8k6iqnvhs-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_klcvbh73so8p-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_ex1fp6bfjdoc-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_ex1fp6bfjdoc-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_7gynq2rc690l-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_7gynq2rc690l-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_ex1fp6bfjdoc-6>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_68op9ugufxnr-5 {
            list-style-type: none
        }

        ul.lst-kix_68op9ugufxnr-4 {
            list-style-type: none
        }

        ul.lst-kix_68op9ugufxnr-3 {
            list-style-type: none
        }

        ul.lst-kix_68op9ugufxnr-2 {
            list-style-type: none
        }

        ul.lst-kix_68op9ugufxnr-1 {
            list-style-type: none
        }

        ul.lst-kix_68op9ugufxnr-0 {
            list-style-type: none
        }

        .lst-kix_ex1fp6bfjdoc-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_7gynq2rc690l-6>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_68op9ugufxnr-8 {
            list-style-type: none
        }

        ul.lst-kix_68op9ugufxnr-7 {
            list-style-type: none
        }

        .lst-kix_q6t6mv5orthl-1>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_68op9ugufxnr-6 {
            list-style-type: none
        }

        .lst-kix_ex1fp6bfjdoc-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_7gynq2rc690l-4>li:before {
            content: "\0025cb  "
        }

        ol.lst-kix_of2uv8ooex53-1.start {
            counter-reset: lst-ctn-kix_of2uv8ooex53-1 0
        }

        ul.lst-kix_klcvbh73so8p-8 {
            list-style-type: none
        }

        .lst-kix_h9ktri7fkm9w-6>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_klcvbh73so8p-7 {
            list-style-type: none
        }

        .lst-kix_rt08gvu9v8s5-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_8i06obw4pk94-3>li:before {
            content: "\0025cf  "
        }

        .lst-kix_h9ktri7fkm9w-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_h9ktri7fkm9w-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_8i06obw4pk94-7>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_klcvbh73so8p-4 {
            list-style-type: none
        }

        ul.lst-kix_klcvbh73so8p-3 {
            list-style-type: none
        }

        .lst-kix_8i06obw4pk94-6>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_klcvbh73so8p-6 {
            list-style-type: none
        }

        .lst-kix_h9ktri7fkm9w-5>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_klcvbh73so8p-5 {
            list-style-type: none
        }

        ul.lst-kix_klcvbh73so8p-0 {
            list-style-type: none
        }

        .lst-kix_rt08gvu9v8s5-4>li:before {
            content: "\0025cb  "
        }

        .lst-kix_rt08gvu9v8s5-5>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_klcvbh73so8p-2 {
            list-style-type: none
        }

        ul.lst-kix_klcvbh73so8p-1 {
            list-style-type: none
        }

        .lst-kix_rt08gvu9v8s5-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_rt08gvu9v8s5-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_8i06obw4pk94-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_68op9ugufxnr-8>li:before {
            content: "\0025a0  "
        }

        .lst-kix_68op9ugufxnr-5>li:before {
            content: "\0025a0  "
        }

        ol.lst-kix_of2uv8ooex53-7.start {
            counter-reset: lst-ctn-kix_of2uv8ooex53-7 0
        }

        .lst-kix_68op9ugufxnr-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_68op9ugufxnr-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_68op9ugufxnr-4>li:before {
            content: "\0025cb  "
        }

        ol.lst-kix_of2uv8ooex53-0.start {
            counter-reset: lst-ctn-kix_of2uv8ooex53-0 0
        }

        ul.lst-kix_xqchyxri23mz-8 {
            list-style-type: none
        }

        ul.lst-kix_xqchyxri23mz-7 {
            list-style-type: none
        }

        ul.lst-kix_xqchyxri23mz-2 {
            list-style-type: none
        }

        ol.lst-kix_of2uv8ooex53-6.start {
            counter-reset: lst-ctn-kix_of2uv8ooex53-6 0
        }

        ul.lst-kix_xqchyxri23mz-1 {
            list-style-type: none
        }

        ul.lst-kix_xqchyxri23mz-0 {
            list-style-type: none
        }

        ul.lst-kix_xqchyxri23mz-6 {
            list-style-type: none
        }

        ul.lst-kix_xqchyxri23mz-5 {
            list-style-type: none
        }

        ul.lst-kix_xqchyxri23mz-4 {
            list-style-type: none
        }

        ul.lst-kix_xqchyxri23mz-3 {
            list-style-type: none
        }

        .lst-kix_fkpbq93k7p5m-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_jw84dwe01i4q-5>li:before {
            content: "\0025a0  "
        }

        .lst-kix_hmrk9a74a5bc-1>li:before {
            content: "\0025cb  "
        }

        .lst-kix_fkpbq93k7p5m-3>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_hmrk9a74a5bc-0 {
            list-style-type: none
        }

        ul.lst-kix_hmrk9a74a5bc-3 {
            list-style-type: none
        }

        ul.lst-kix_hmrk9a74a5bc-4 {
            list-style-type: none
        }

        ul.lst-kix_hmrk9a74a5bc-1 {
            list-style-type: none
        }

        ul.lst-kix_hmrk9a74a5bc-2 {
            list-style-type: none
        }

        .lst-kix_jw84dwe01i4q-1>li:before {
            content: "\0025cb  "
        }

        ul.lst-kix_hmrk9a74a5bc-7 {
            list-style-type: none
        }

        ul.lst-kix_hmrk9a74a5bc-8 {
            list-style-type: none
        }

        .lst-kix_hmrk9a74a5bc-5>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_hmrk9a74a5bc-5 {
            list-style-type: none
        }

        ul.lst-kix_hmrk9a74a5bc-6 {
            list-style-type: none
        }

        .lst-kix_98g8k6iqnvhs-7>li:before {
            content: "\0025cb  "
        }

        .lst-kix_jx3xljry6r5h-6>li:before {
            content: "\0025cf  "
        }

        .lst-kix_98g8k6iqnvhs-3>li:before {
            content: "\0025cf  "
        }

        ol.lst-kix_of2uv8ooex53-5.start {
            counter-reset: lst-ctn-kix_of2uv8ooex53-5 0
        }

        .lst-kix_klcvbh73so8p-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_j3gz6m1fbjdi-0>li:before {
            content: "\0025cf  "
        }

        .lst-kix_jx3xljry6r5h-2>li:before {
            content: "\0025a0  "
        }

        .lst-kix_q6t6mv5orthl-2>li:before {
            content: "\0025a0  "
        }

        ul.lst-kix_2odvspw48wt1-0 {
            list-style-type: none
        }

        ul.lst-kix_2odvspw48wt1-1 {
            list-style-type: none
        }

        ul.lst-kix_2odvspw48wt1-2 {
            list-style-type: none
        }

        ul.lst-kix_2odvspw48wt1-3 {
            list-style-type: none
        }

        .lst-kix_q6t6mv5orthl-6>li:before {
            content: "\0025cf  "
        }

        ul.lst-kix_2odvspw48wt1-4 {
            list-style-type: none
        }

        ul.lst-kix_2odvspw48wt1-5 {
            list-style-type: none
        }

        ul.lst-kix_2odvspw48wt1-6 {
            list-style-type: none
        }

        ul.lst-kix_2odvspw48wt1-7 {
            list-style-type: none
        }

        ul.lst-kix_2odvspw48wt1-8 {
            list-style-type: none
        }

        ol.lst-kix_of2uv8ooex53-2.start {
            counter-reset: lst-ctn-kix_of2uv8ooex53-2 0
        }

        .lst-kix_klcvbh73so8p-6>li:before {
            content: "\0025cf  "
        }

        ol.lst-kix_of2uv8ooex53-5 {
            list-style-type: none
        }

        ol.lst-kix_of2uv8ooex53-4 {
            list-style-type: none
        }

        ol.lst-kix_of2uv8ooex53-3 {
            list-style-type: none
        }

        ol.lst-kix_of2uv8ooex53-2 {
            list-style-type: none
        }

        .lst-kix_ex1fp6bfjdoc-3>li:before {
            content: "\0025cf  "
        }

        ol.lst-kix_of2uv8ooex53-8 {
            list-style-type: none
        }

        ol.lst-kix_of2uv8ooex53-7 {
            list-style-type: none
        }

        ol.lst-kix_of2uv8ooex53-6 {
            list-style-type: none
        }

        .lst-kix_ex1fp6bfjdoc-7>li:before {
            content: "\0025cb  "
        }

        ol.lst-kix_of2uv8ooex53-1 {
            list-style-type: none
        }

        .lst-kix_7gynq2rc690l-1>li:before {
            content: "\0025cb  "
        }

        ol.lst-kix_of2uv8ooex53-0 {
            list-style-type: none
        }

        li.li-bullet-0:before {
            margin-left: -18pt;
            white-space: nowrap;
            display: inline-block;
            min-width: 18pt
        }

        .lst-kix_7gynq2rc690l-5>li:before {
            content: "\0025a0  "
        }

        ol.lst-kix_of2uv8ooex53-3.start {
            counter-reset: lst-ctn-kix_of2uv8ooex53-3 0
        }

        ol {
            margin: 0;
            padding: 0
        }

        table td,
        table th {
            padding: 0
        }

        .c1 {
            border-right-style: solid;
            padding-top: 0pt;
            border-top-width: 0pt;
            border-right-width: 0pt;
            padding-left: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            border-left-width: 0pt;
            border-top-style: solid;
            background-color: #ffffff;
            margin-left: 36pt;
            border-left-style: solid;
            border-bottom-width: 0pt;
            border-bottom-style: solid;
            orphans: 2;
            widows: 2;
            text-align: justify;
            padding-right: 0pt;
            height: 11pt
        }

        .c37 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1.5pt;
            border-right-width: 1.5pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1.5pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 535.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c33 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1.5pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1.5pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1.5pt;
            width: 248.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c23 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            /* border-bottom-color: #000000; */
            /* border-top-width: 1pt; */
            /* border-right-width: 1pt; */
            /* border-left-color: #000000; */
            /* vertical-align: top; */
            /* border-right-color: #000000; */
            /* border-left-width: 1pt; */
            /* border-top-style: solid; */
            /* border-left-style: solid; */
            /* border-bottom-width: 1pt; */
            width: 535.5pt;
            /* border-top-color: #000000; */
            /* border-bottom-style: solid */
        }

        .c43 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 252.8pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c25 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1.5pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1.5pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1.5pt;
            width: 535.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c49 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1.5pt;
            border-right-width: 1.5pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1.5pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 248.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }

        .c21 {
            border-right-style: solid;
            border-top-width: 0pt;
            border-right-width: 0pt;
            border-left-width: 0pt;
            border-top-style: solid;
            background-color: #ffffff;
            border-left-style: solid;
            border-bottom-width: 0pt;
            border-bottom-style: solid;
            padding-right: 0pt
        }

        .c27 {
            background-color: #ffffff;
            color: #212121;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 11pt;
            font-family: "Arial";
            font-style: normal
        }

        .c46 {
            margin-left: 36pt;
            padding-top: 0pt;
            padding-left: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        .c2 {
            margin-left: 36pt;
            padding-top: 0pt;
            padding-left: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .c28 {
            padding-top: 18pt;
            padding-bottom: 6pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify;
            height: 16pt
        }

        .c11 {
            padding-top: 20pt;
            padding-bottom: 6pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .c22 {
            padding-top: 16pt;
            padding-bottom: 4pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .c44 {
            color: #000000;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 24pt;
            font-family: "Droid Serif";
            font-style: normal
        }

        .c30 {
            padding-top: 18pt;
            padding-bottom: 6pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .c6 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: justify;
            height: 11pt
        }

        .c9 {
            color: #434343;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 14pt;
            font-family: "Arial";
            font-style: normal
        }

        .c36 {
            color: #000000;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 26pt;
            font-family: "Arial";
            font-style: normal
        }

        .c16 {
            color: #666666;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 12pt;
            font-family: "Arial";
            font-style: normal
        }

        .c38 {
            padding-top: 14pt;
            padding-bottom: 4pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .c12 {
            color: #000000;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 20pt;
            font-family: "Arial";
            font-style: normal
        }

        .c15 {
            color: #000000;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 16pt;
            font-family: "Arial";
            font-style: normal
        }

        .c41 {
            color: #202124;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 11pt;
            font-family: "Arial";
            font-style: normal
        }

        .c29 {
            padding-top: 20pt;
            padding-bottom: 6pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        .c4 {
            padding-top: 12pt;
            padding-bottom: 4pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .c0 {
            color: #666666;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 11pt;
            font-family: "Arial";
            font-style: normal
        }

        .c55 {
            color: #000000;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 11pt;
            font-family: "PT Serif";
            font-style: normal
        }

        .c10 {
            color: #000000;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 11pt;
            font-family: "Arial";
            font-style: normal
        }

        .c42 {
            padding-top: 16pt;
            padding-bottom: 4pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        .c20 {
            padding-top: 0pt;
            padding-bottom: 3pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: center
        }

        .c17 {
            padding-top: 0pt;
            padding-bottom: 3pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .c45 {
            color: #000000;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 11pt;
            font-family: "Arial";
            font-style: normal
        }

        .c24 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: center
        }

        .c32 {
            color: #000000;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 20pt;
            font-family: "Arial";
            font-style: normal
        }

        .c19 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .c48 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        .c51 {
            background-color: #ffffff;
            font-size: 10.5pt;
            font-family: "Roboto";
            color: #333333;
            font-weight: 400
        }

        .c34 {
            text-decoration-skip-ink: none;
            -webkit-text-decoration-skip: none;
            color: #0000ee;
            text-decoration: underline
        }

        .c26 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.0;
            text-align: center
        }

        .c54 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.0;
            text-align: left
        }

        .c5 {
            text-decoration-skip-ink: none;
            -webkit-text-decoration-skip: none;
            color: #1155cc;
            text-decoration: underline
        }

        .c53 {
            margin-left: auto;
            border-spacing: 0;
            border-collapse: collapse;
            margin-right: auto
        }

        .c18 {
            border-spacing: 0;
            border-collapse: collapse;
            margin-right: auto
        }

        .c14 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.0;
            text-align: justify
        }

        .c50 {
            background-color: #ffffff;
            max-width: 535.5pt;
            padding: 27pt 36pt 45pt 40.5pt
        }

        .c40 {
            background-color: #ffffff;
            color: #202124
        }

        .c35 {
            padding: 0;
            margin: 0
        }

        .c8 {
            color: inherit;
            text-decoration: inherit
        }

        .c47 {
            background-color: #ffffff;
            color: #212121
        }

        .c39 {
            height: 11pt
        }

        .c7 {
            font-weight: 700
        }

        .c3 {
            height: 0pt
        }

        .c13 {
            text-indent: 36pt
        }

        .c52 {
            padding-left: 0pt
        }

        .c31 {
            height: 20pt
        }

        .title {
            padding-top: 0pt;
            color: #000000;
            font-size: 26pt;
            padding-bottom: 3pt;
            font-family: "Arial";
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            /* text-align: left */
        }

        .subtitle {
            padding-top: 0pt;
            color: #666666;
            font-size: 15pt;
            padding-bottom: 16pt;
            font-family: "Arial";
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        li {
            color: #000000;
            font-size: 11pt;
            font-family: "Arial"
        }

        p {
            margin: 0;
            color: #000000;
            font-size: 11pt;
            font-family: "Arial"
        }

        h1 {
            padding-top: 20pt;
            color: #000000;
            font-size: 20pt;
            padding-bottom: 6pt;
            font-family: "Arial";
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: center
        }

        h2 {
            padding-top: 18pt;
            color: #000000;
            font-size: 16pt;
            padding-bottom: 6pt;
            font-family: "Arial";
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        h3 {
            padding-top: 16pt;
            color: #434343;
            font-size: 14pt;
            padding-bottom: 4pt;
            font-family: "Arial";
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        h4 {
            padding-top: 14pt;
            color: #666666;
            font-size: 12pt;
            padding-bottom: 4pt;
            font-family: "Arial";
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        h5 {
            padding-top: 12pt;
            color: #666666;
            font-size: 11pt;
            padding-bottom: 4pt;
            font-family: "Arial";
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        h6 {
            padding-top: 12pt;
            color: #666666;
            font-size: 11pt;
            padding-bottom: 4pt;
            font-family: "Arial";
            line-height: 1.15;
            page-break-after: avoid;
            font-style: italic;
            orphans: 2;
            widows: 2;
            text-align: left
        }
    </style>
</head>

<body class="c50 doc-content">
    <p class="c6"><span class="c10"></span></p><a id="t.2ac2ac5ce2570fe20a2e9431a13c9485078ae623"></a><a id="t.0"></a>
    <table class="c53">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <!-- <p class="c17 title" id="h.osgm8w4lrbnx"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 313.21px; height: 204.10px;"><img
                            alt="" src="images/image29.png"
                            style="width: 313.21px; height: 204.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p> -->
                <p class="c20 title" id="h.twemqbx3y2ca"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 313.08px; height: 207.78px;"><img
                            alt="" src="images/image26.jpg"
                            style="width: 313.08px; height: 207.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
                <p class="c20 title" id="h.glpu53fzyulb"><span class="c44">Voice Emotion Detection</span></p>
                <p class="c20 title" id="h.noahzpny755g"><span class="c44">Project Report</span></p>
                <p class="c6"><span class="c10"></span></p>
            </td>
        </tr>
    </table>
    <p class="c6"><span class="c10"></span></p>
    <h1 class="c11" id="h.qswft3gaumtb"><span class="c7">Abstract </span></h1>
    <p class="c19 c13"><span class="c10">In this project of voice emotion detection, we have used two audio datasets
            Ravdess and Crema-D on three different machine learning models: Convolutional Neural Networks (CNN),
            Recurrent Neural Networks (RNN) and Transfer learning. Our main goal in this project is to collect the
            tagged data from the user of our app. We also </span></p>
    <p class="c19"><span>focused on building as accurate a model as possible. This way more users will use our app and
            our final goal to collect more data is possible. Our app Voice Emotion Detection (VEM) was developed like a
            game so users would want to play.. In the future, there will be a &nbsp;large number of applications under
            the development phase that are based on emotion. Robots in different fields need to understand human
            feelings so they can respond based on the mood at a particular moment. The main problem for these
            applications is lack of labeled data. Our project is helpful for a large number of applications that are
            struggling due to data and will cause Significant improvements in the performance of all apps. </span></p>
    <p class="c21 c19 c13 c52"><span>This project allows users to record audio of themselves and get an output of the
            detected emotion. They then select if that was the correct emotion or if they are really feeling something
            else. By doing this, the user has a fun game to see if their emotion can be detected by an app, and machine
            learning engineers will be gaining tagged emotion audio files to use for future projects that build off of
            this one. </span></p>
    <h1 class="c11" id="h.9yvnrub5rav"><span class="c32 c7">Introduction </span></h1>
    <p class="c19 c13"><span class="c10">Voice emotion detection is a task to detect human emotions from audio signals.
            This is very important in advancing human-computer interactions because large amounts of information are
            transferred by audio, with the audio also containing features like emotion. &nbsp;Understanding one&rsquo;s
            feelings while communicating is constructive in comprehending the conversation and responding appropriately.
            Currently the interaction between humans and computers is not very good. The reason for low accuracy in
            these applications is that we have a lack of labeled data. </span></p>
    <p class="c19 c13"><span>Voice emotion detection helps smart speakers and virtual assistants to understand their
            user better. This is also helpful for online interactive tutorials. This can be used in more areas like:
            vehicles to know the driver&#39;s emotion to avoid the accidents, smart robots, security, and therapy
            sessions to understand the patient&#39;s feelings. All these applications mentioned above currently do not
            perform well. This is because we don&#39;t have large tagged data for them to learn from. Our app will
            collect the tagged &nbsp;data from all &nbsp;users across the globe, &nbsp;then will help in increasing the
            performance of all the apps that are currently performing poorly and some future apps &nbsp;based on human
            emotion.</span></p>
    <h1 class="c11" id="h.dzugiv9ve9k1"><span class="c32 c7">Background </span></h1>
    <p class="c19 c13"><span class="c10">There are lots of applications already existing based on human emotion
            detection by machine learning with different models. If we talk about the future, then humans in some jobs
            like restaurant servers and assistants can be replaced by smart robots. Making this a possibility with high
            accuracy depends on having lots of quality data. There are some robots that &nbsp;are not yet fully
            developed and emotion detection is one component of them that still needs to be developed. Below are some
            examples of these robots: </span></p>
    <p class="c6 c13"><span class="c10"></span></p><a id="t.3ffdd4726878c054e46dd4f36581efa21745a8ba"></a><a
        id="t.1"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 310.30px; height: 242.27px;"><img
                            alt="" src="images/image27.jpg"
                            style="width: 310.30px; height: 242.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c24"><span class="c10">1. Pepper as receptionist at Bank.</span></p>
            </td>
        </tr>
    </table>
    <p class="c6"><span class="c10"></span></p><a id="t.289e8aeb2b7af9be705ad044fcc978f7e60dd01e"></a><a id="t.2"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.66px; height: 207.50px;"><img
                            alt="" src="images/image36.jpg"
                            style="width: 312.66px; height: 207.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c24"><span class="c10">2. &nbsp;Snow talking to lonely women.</span></p>
                <p class="c26 c39"><span class="c10"></span></p>
            </td>
        </tr>
    </table>
    <p class="c24 c39"><span class="c10"></span></p><a id="t.573034463e3ea8918c0b6830cfaaa6117acc6bf7"></a><a
        id="t.3"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 293.50px; height: 164.82px;"><img
                            alt="" src="images/image12.jpg"
                            style="width: 293.50px; height: 164.82px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c24"><span class="c10">3. Paro for children and elders.</span></p>
            </td>
        </tr>
    </table>
    <p class="c24 c39"><span class="c10"></span></p><a id="t.0852c0e6b00fa970054258738114bbd203ddd29b"></a><a
        id="t.4"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 303.50px; height: 236.97px;"><img
                            alt="" src="images/image16.jpg"
                            style="width: 303.50px; height: 236.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c24"><span class="c10">4. Surena IV for speech recognition.</span></p>
            </td>
        </tr>
    </table>
    <p class="c6 c13"><span class="c10"></span></p>
    <h1 class="c11" id="h.m8taq1jh5k8u"><span class="c32 c7">Datasets</span></h1>
    <h3 class="c42" id="h.pzjchi44lvnb"><span class="c9">Ryerson Audio-Visual Database of Emotional Speech and Song
            (RAVDESS)</span></h3>
    <p class="c13 c19"><span>Audio-only files of all actors (01-24) are available as 2 separate zip files (</span><span
            class="c51">16bit, 48kHz .wav </span><span class="c10">~200 MB each):</span></p>
    <p class="c6"><span class="c10"></span></p>
    <ul class="c35 lst-kix_jklb5xavpn79-0 start">
        <li class="c46 li-bullet-0"><span>Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60
                trials per actor x 24 actors = 1440:</span><span class="c5"><a class="c8"
                    href="https://www.google.com/url?q=https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio&amp;sa=D&amp;source=editors&amp;ust=1657674612364092&amp;usg=AOvVaw3oYTp_bEqL7TpXcB8LPh1k">https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio</a></span>
        </li>
    </ul>
    <p class="c39 c48"><span class="c10"></span></p>
    <ul class="c35 lst-kix_jklb5xavpn79-0">
        <li class="c46 li-bullet-0"><span>Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials
                per actor x 23 actors = 1012: </span><span class="c5"><a class="c8"
                    href="https://www.google.com/url?q=https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-song-audio&amp;sa=D&amp;source=editors&amp;ust=1657674612364588&amp;usg=AOvVaw2xZDlFpCTDmY46xwobcM85">https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-song-audio</a></span>
        </li>
    </ul>
    <p class="c6"><span class="c10"></span></p>
    <p class="c19 c13"><span class="c10">The database contains 24 professional actors (12 female, 12 male), vocalizing
            two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad,
            angry, fearful, surprise, and disgust expressions, and the song contains calm, happy, sad, angry, and
            fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an
            additional neutral expression.</span></p>
    <p class="c6"><span class="c10"></span></p>
    <h3 class="c42" id="h.o61iwk4rkc4b"><span>Crowd Sourced Emotional Multimodal Actors Dataset (</span><span
            class="c9">CREMA-D)</span></h3>
    <p class="c19 c13"><span class="c10">CREMA-D is a data set of 7,442 original clips from 91 actors. These clips were
            from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and
            ethnicities (African American, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection
            of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear,
            Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).</span></p>
    <p class="c19"><span class="c5"><a class="c8"
                href="https://www.google.com/url?q=https://github.com/CheyneyComputerScience/CREMA-D/tree/master/AudioWAV&amp;sa=D&amp;source=editors&amp;ust=1657674612365540&amp;usg=AOvVaw2jC7v6qjauC14dfd8M-Kow">https://github.com/CheyneyComputerScience/CREMA-D/tree/master/AudioWAV</a></span>
    </p>
    <h3 class="c42" id="h.fu3oau2ngq3g"><span>Toronto emotional speech set (</span><span class="c9">TESS)</span></h3>
    <p class="c19 c13"><span class="c10">A set of 200 target words were spoken in the carrier phrase &quot;Say the word
            _&#39; by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven
            emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data
            points (audio files) in total.</span></p>
    <p class="c19 c13"><span class="c10">The dataset is organized such that each of the two female actresses and their
            emotions are contained within its own folder. And within that, all 200 target words audio files can be
            found. The format of the audio file is a WAV format.</span></p>
    <p class="c19"><span class="c5"><a class="c8"
                href="https://www.google.com/url?q=https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess&amp;sa=D&amp;source=editors&amp;ust=1657674612366321&amp;usg=AOvVaw1jj4CguwK5qrCdEJxRjd6l">https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess</a></span>
    </p>
    <h1 class="c11" id="h.gjeifqp2mrkw"><span class="c7 c32">Methods</span></h1>
    <h2 class="c30" id="h.w804r9n75d5k"><span class="c15">CNN</span></h2>
    <h3 class="c22" id="h.hvvopf7wemov"><span class="c9">Data Preparation</span></h3>
    <p class="c19"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First, we combined all 4 datasets (RAVDESS
            Speech, </span><span>RAVDESS</span><span class="c10">&nbsp;Song, CREMA-D, TESS) into one big dataset. Next,
            we relabeled &ldquo;calm&rdquo; data to &ldquo;neutral&rdquo; because the two classes of data do not have
            many differences. Then, among 4 datasets, only 2 datasets have &ldquo;surprised&rdquo; class, so the data
            within the &ldquo;surprised&rdquo; class became the minority which needs more augmentation. For each raw
            datapoint of all classes except surprised, we augmented the original voice data by creating more voice
            types. We did this by randomly adding 1 more voice with noise, 1 more voice with random pitch, 1 voice with
            random speed, 1 voice with shifted starting points, and 1 voice with changed speed and pitch. For the
            datapoint of class &ldquo;surprised&rdquo;, instead of adding 1 more voice for each type, we added 4 more.
            After data augmentation, we got a total of 90,090 samples equally distributed in 7 classes, as shown in
            figure 1.</span></p>
    <p class="c6"><span class="c10"></span></p><a id="t.1e1f4863998ca946825e6fd3beb204a969d1e15a"></a><a id="t.5"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 305.50px; height: 162.38px;"><img
                            alt="" src="images/image38.png"
                            style="width: 305.50px; height: 162.38px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7 c47">Figure 1</span><span class="c27">: Sample distribution of 7 classes
                        in 4 dataset combined with data augmentation</span></p>
            </td>
        </tr>
    </table>
    <p class="c6 c13"><span class="c10"></span></p>
    <p class="c19 c13"><span>F</span><span class="c10">or each audio datapoint (including the augmented data), we
            extracted 9 features: mfccs, chroma, mel_spectrogram, spectral_contrast, rms, spectral_centroid,
            &nbsp;spectral_bandwidth, flatness, and rolloff. Feature definitions are explained as below:</span></p>
    <p class="c6 c13"><span class="c10"></span></p>
    <ul class="c35 lst-kix_sfxyl3uwwh5k-0 start">
        <li class="c2 li-bullet-0"><span class="c7">Mel-Frequency Cepstral Coefficients (MFCCs)</span><span
                class="c10">:The information of the rate of change in spectral bands of a signal is given by its
                cepstrum. &nbsp;A cepstrum is basically a spectrum of the log of the spectrum of the time signal.The
                cepstrum conveys the different values that construct the formants (a characteristic component of the
                quality of a speech sound) and timbre of a sound. </span></li>
        <li class="c2 li-bullet-0"><span class="c7">Chroma</span><span class="c10">: Calculates how much of each
                chromatic pitch class exists in the signal. Often used to analyze the harmonic content of recorded
                music, such as in chord or key detection.</span></li>
        <li class="c2 li-bullet-0"><span class="c7">Mel spectrogram</span><span class="c10">: A mel-spectrogram is a
                spectrogram where the frequencies are converted to the mel scale.A spectrogram is a visual depiction of
                the spectrum of frequencies of an audio signal as it varies with time. Hence it includes both time and
                frequency aspects of the signal.</span></li>
        <li class="c2 li-bullet-0"><span class="c7">Spectral_contrast</span><span class="c10">: the measure of the
                energy of frequency at each timestamp.</span></li>
        <li class="c2 li-bullet-0"><span class="c7">Root mean squared (rms)</span><span class="c10">: Root Mean Square
                Energy is based on all samples in a frame. It acts as an indicator of loudness, since higher the energy,
                louder the sound. This feature has been useful in audio segmentation and music genre classification
                tasks.</span></li>
        <li class="c2 li-bullet-0"><span class="c7">Spectral_centroid</span><span class="c10">: The Spectral Centroid
                provides the center of gravity of the magnitude spectrum. In other words, it gives the frequency band
                where most of the energy is concentrated. It maps into a very prominent timbral feature called
                &quot;brightness of sound&quot; (energetic, open, dull).This can be used for example to classify a bass
                guitar (low spectral centroid) from a trumpet (high spectral centroid).</span></li>
        <li class="c2 li-bullet-0"><span class="c7">Spectral_bandwidth</span><span class="c10">: The spectral bandwidth
                or spectral spread is derived from the spectral centroid. It is the spectral range of interest around
                the centroid, that is, the variance from the spectral centroid.</span></li>
        <li class="c2 li-bullet-0"><span class="c7">Flatness</span><span class="c10">: The flatness of the spectrum. It
                is computed using the ratio between the geometric and arithmetic means. It determines how noisy a sound
                is. For example a pure sine wave will have a flatness that approaches 0.0, and white noise will have a
                flatness that approaches 1.0</span></li>
        <li class="c2 li-bullet-0"><span class="c7">Rolloff</span><span class="c10">: The frequency below which contains
                99% of the energy of the spectrum.an be used to approximate the maximum frequency in a signal.</span>
        </li>
    </ul>
    <p class="c6 c13"><span class="c10"></span></p>
    <p class="c19 c13"><span class="c10">All 9 features are concatenated together to form a feature frame, which was
            then resized to shape (90,90) by interpolating to make sure all feature frames have the same shape (figure
            2). These feature frames were fed into the CNN model with below architecture (figure 3).</span></p><a
        id="t.2c5652ebac2613b8cf706fd3f9017f839ba1bb46"></a><a id="t.6"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 321.50px; height: 249.00px;"><img
                            alt="" src="images/image33.png"
                            style="width: 321.50px; height: 249.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 2</span><span class="c10">: Each raw voice data
                        &ldquo;produced&rdquo; 6 more voice data types (resized raw voice, resized with noise, resized
                        with new pitch, resized with new speed, resized with random time shift, and resized with new
                        pitch &amp; speed combined. Features are extracted and concatenated to form a feature frame for
                        each data point. All feature frames are then fed to the CNN model.</span></p>
            </td>
        </tr>
    </table>
    <h3 class="c22" id="h.1wwcn2es5rg6"><span class="c9">Training</span></h3>
    <p class="c19 c13"><span>On the first training time, we did not fix the data imbalancing issue on the
            &ldquo;surprised&rdquo; class and put everything into training the model. </span><span>We split the data to
            ratio 80% training:10% testing:10% validation. Therefore, on the first training time, 64,968 samples were
            randomly chosen for training, 8,121 samples went to testing and 8,121 to validating. After augmenting more
            data of the &ldquo;surprised&rdquo; class, 72,072 randomly chosen samples went to training,
            &nbsp;</span><span class="c47">9,009 went to testing and 9,009 to validating. </span><span
            class="c10">Figure 3 shows that this model uses the ReLU function for the activation to overcome the problem
            of vanishing the gradient. We also did the batch normalization twice: first was the initial layer of the CNN
            and the second was the center of the model. Our model converged at 800 epochs with a batch size of 100,
            using sparse categorical cross entropy loss and an &lsquo;RMSprop&rsquo; optimizer. Each epoch took around 9
            seconds.</span></p>
    <p class="c6 c13"><span class="c10"></span></p><a id="t.3b448bb6688eaac0e104ff1d27919284918151d2"></a><a
        id="t.7"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 313.48px; height: 200.51px;"><img
                            alt="" src="images/image13.png"
                            style="width: 313.48px; height: 200.51px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 279.50px; height: 430.94px;"><img
                            alt="" src="images/image39.png"
                            style="width: 279.50px; height: 435.73px; margin-left: 0.00px; margin-top: -4.79px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 3</span><span class="c10">: Custom CNN model architecture.</span>
                </p>
            </td>
        </tr>
    </table>
    <h2 class="c30" id="h.5k9wyolup17i"><span class="c15">Transfer Learning</span></h2>
    <p class="c19 c13"><span class="c10">The Purpose of Transfer Learning is to use a pre-trained model that is trained
            on a different task, but hopefully similar to the one you wish to perform, and uses it as a starting point
            for training a new task. By doing this, we are able to train significantly faster than new models and
            achieve higher accuracy with less data. For this project, we tried out three methods for transfer learning:
            VGG16 with raw spectrograms, VGG19 with augmented spectrograms and VGG19 with the four augmented datasets.
            We chose to stick with VGG16 (and the added layers of VGG19) because we planned to convert all audio to
            spectrograms or other visual representations of audio and this model has been trained on a very robust
            dataset of images: imagenet. Although imagenet does not contain any visual representations of audio, it does
            have 14 million images belonging to about 20,000 classes, making it the most diverse image trained model we
            have access to. </span></p>
    <h3 class="c22" id="h.xbds3zdmeeif"><span class="c9">Raw VGG16</span></h3>
    <p class="c19"><span class="c10">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For this baseline method, we used
            the RAVDESS Speech dataset, converted each audio file into a mel Spectrogram, and fed these into the updated
            VGG16 model. We did not perform any other augmentation or feature extraction to see a baseline of
            performance on our very raw singular dataset. We froze everything except the three fully-connected top
            layers. To replace those, we added custom flatten, dense, and dropout layers. The dense layer used relu
            activation and dropout of 0.4, meaning 40% of the neurons are dropped, ideally to prevent overfitting.
            Finally a dense layer with eight classes, one for each emotion, and softmax activation was put on top.
            Categorical cross entropy was used for the loss because the labels were one hot encoded before training,
            Adam as the optimizer, and accuracy the metric to judge performance of the model. We also used the imagenet
            weights, which are the learned weights from the model being fed the imagenet dataset. We first ran 10 epochs
            and did not see much improvement in accuracy, so we then ran 50 epochs and had the same outcome. What is
            interesting is the model predicted all samples as disgust, even though there was no unbalance towards
            disgust, but disgust was the first emotion fed into the model. The mel spectrogram only extracts the
            frequency of the audio signal, giving the model only that one feature to look at. The highest accuracy
            achieved was 22%, with an average closer to 10%. </span></p>
    <p class="c6 c13"><span class="c10"></span></p><a id="t.88d4998b1f84a8e53fc5eb4bdd5ad44797456789"></a><a
        id="t.8"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c37" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 309.83px; height: 242.84px;"><img
                            alt="" src="images/image34.png"
                            style="width: 309.83px; height: 349.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 300.46px; height: 284.15px;"><img
                            alt="" src="images/image25.png"
                            style="width: 300.46px; height: 371.58px; margin-left: 0.00px; margin-top: -87.43px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c25" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 4</span><span class="c10">: Raw VGG16 architecture </span></p>
            </td>
        </tr>
    </table>
    <h3 class="c22" id="h.gl1cvww1hwv5"><span class="c9">First VGG19</span></h3>
    <p class="c19"><span class="c10">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For this model, we first started by
            augmenting the data. We created spectrograms of altered speed and pitch to add more noise and robust samples
            to the sparse dataset. We then decided to add more layers in the hope of being able to extract more features
            and prevent early overfitting. To do this, we froze the first 19 layers of VGG16, using the same imagenet
            weights, input shape, and removal of fully-connected layers. We then added the same custom layers to the
            top: flatten, dense, dropout, along with another dense using relu activation and another dense using softmax
            activation for the multiclass nature of this model. We used the same loss, optimizer, and metrics, but ran
            100 epochs to try and achieve much higher accuracy as this model was showing good improvements. </span></p>
    <p class="c19 c13"><span class="c10">This version still strongly favored one category over the other, this time
            being calm rather than disgust. Just by adding speed and pitch changes to the mel spectrograms, causing more
            variance within the categories, we were able to have a lot of improvements over just the raw data. The main
            downfall of this model is the early onset of overfitting. Although we added more samples and diversity,
            having only one feature caused the model to align too closely to the calm category, being that it was the
            first one learnt, and was unable to distinguish the other categories from it. The accuracy on the test data
            was 33%. </span></p>
    <p class="c6 c13"><span class="c10"></span></p><a id="t.69503c9ab1ad6ef68698c078664436c201bd8479"></a><a
        id="t.9"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c37" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 182.24px; height: 349.50px;"><img
                            alt="" src="images/image4.png"
                            style="width: 182.24px; height: 349.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c25" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 5</span><span class="c10">: First VGG19 architecture and
                        trainability</span></p>
            </td>
        </tr>
    </table>
    <h3 class="c22" id="h.xklki5us023r"><span class="c9">Second VGG19</span></h3>
    <p class="c19"><span class="c10">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For our final attempt at transfer
            learning for this problem, we changed several things. First, we used the four combined and augmented
            datasets developed for the CNN model. To do this, we had to change the dimensions of our input. VGG16
            trained on imagement expects the base input to be in the format [none, 224,224,3], with the 224 being
            swapped out by your custom input image size and none being the number of samples. The combined data is in
            the form of text representing five features of the audio files. These arrays did not have 3 dimensions,
            given that they don&rsquo;t have rgb channels, so we had to expand the last dimension of the array to be 1.
            We also had to change the model to not use imagenet weights because the input sizes are so different.
            Instead, we did not use any pretrained frozen weights. We also changed the loss metric to be sparse
            categorical cross entropy, because the labels for this combined dataset are integers. We were able to run 15
            epochs, each needing about 30 minutes to run, and saw much better performance than either previous model.
        </span></p>
    <p class="c19"><span class="c10">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To cut down on the overfitting, we
            used the combined dataset and removed the calm class given that it was hard for us to distinguish calm from
            neutral. After just 15 epochs, the model is showing good improvements and not overfitting nearly as bad as
            the previous model. The main constraint here is the training time. Each epoch takes about 30 minutes to
            complete because of the number of features, size of the dataset, and the fact that we could no longer use
            pretrained weights with this new input shape, so each layer was initialized with random weights and trained
            from scratch. The first epochs accuracy was 19% and the final accuracy achieved was 45% on epoch 15. </span>
    </p>
    <p class="c6 c13"><span class="c10"></span></p><a id="t.09ab7902e7d0dd4190ac64f5b92fa6409c4c2101"></a><a
        id="t.10"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c37" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 243.00px; height: 438.00px;"><img
                            alt="" src="images/image11.png"
                            style="width: 246.73px; height: 873.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 261.50px; height: 440.00px;"><img
                            alt="" src="images/image11.png"
                            style="width: 261.50px; height: 870.49px; margin-left: 0.00px; margin-top: -430.49px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c25" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 6</span><span class="c10">: Final VGG19 architecture</span></p>
            </td>
        </tr>
    </table>
    <h2 class="c28" id="h.h0uf0qyajm5"><span class="c15"></span></h2>
    <h2 class="c30" id="h.xv7ro1uaim2o"><span class="c15">LSTM</span></h2>
    <p class="c19 c13"><span class="c10">Recurrent neural networks (RNN) rely on creating a hidden state that&rsquo;s
            consistent through the length of a sample split into ordered subsections, usually sentences being the sample
            and words being the subsections. The model reads the information from the sample sequentially. Since sound
            data is a time series data, we thought that an RNN could perform well at classifying sound. Thus, we went in
            the direction of using an RNN. Specifically, we looked at using a Long Short Term Memory (LSTM) model as it
            has been shown to perform better due to its longer term memory capabilities.</span></p>
    <p class="c19 c13"><span class="c10">When sound is recorded, the data captured is a list of pressure values over
            time. The pressure differences are usually very large numbers and it&#39;s common to take the log of those
            values to turn them into what&#39;s known as decibels. In order to classify sound information, we need the
            model to look at the same information a human would notice about these pressure waves. Humans make sense of
            sounds by noticing a subset of frequencies over time. Frequencies being the amount of times a pressure peak
            occurs per second. We can thus choose to look at frequencies over time rather than pressure over time. We
            create a 2D image with the y-axis corresponding to frequency, mel-frequency, or any other feature that can
            be extracted from the pressure wave as a function of time. The x-axis corresponds to time.</span></p>
    <p class="c19 c13"><span class="c10">To create the series of data needed for an RNN, we then split the 2D image
            representation of the sound recordings from the CREMA_D dataset into subsection of MxN windows, where M is
            the number of milliseconds in the window and N is the whole frequency/ feature range. We will then have
            multiple windows over time per recording for an RNN model to train and predict over.</span></p>
    <h3 class="c22" id="h.78iw1wvigvjm"><span class="c9">ConvLSTM2D</span></h3>
    <p class="c19 c13"><span class="c10">Since we are looking at sequences of images that represent the sound data. We
            use convolutional LSTMs a.k.a ConvLSTM2D to use the image information appropriately. This layer type is a
            mix of a convolutional layer with an LSTM layer. This layer learns the kernels to use on an image when
            running convolutions to create a hidden state that over the subsections of the sample is adjusted to predict
            a classification on the sound. </span></p><a id="t.b4be2ade8867ae08aafaeaf5b3befc00c6773544"></a><a
        id="t.11"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 316.57px; height: 356.50px;"><img
                            alt="" src="images/image22.png"
                            style="width: 316.57px; height: 356.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 7</span><span class="c10">: The Convolutional LSTM 2D model
                        architecture. The layers consist of an input layer followed by a Convolutional LSTM with 8
                        features, a Convolutional LSTM with 16 features, a batch normalization, flattening, a fully
                        connected 20, and a fully connected 6 as the output. For both of the ConvLSTM2Ds we used a
                        dropout of 0.2, a randomly generated bias over a normal distribution, and padding
                        &lsquo;same&rsquo;.</span></p>
            </td>
        </tr>
    </table>
    <h4 class="c38" id="h.myey09t0vmsf"><span class="c16">Data Preparation</span></h4>
    <p class="c19 c13"><span class="c10">We first convert the pressure vs time data recorded as a .wav file into a
            mel-spectrogram as shown in figure x. To do this we use librosa, a python library for sound analysis.</span>
    </p>
    <p class="c6 c13"><span class="c10"></span></p>
    <p class="c6 c13"><span class="c10"></span></p>
    <p class="c6 c13"><span class="c10"></span></p>
    <p class="c6"><span class="c10"></span></p><a id="t.66521092ab054ee1c4d8029ea0f2bcc596f0fe37"></a><a id="t.12"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c43" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 315.98px; height: 95.10px;"><img
                            alt="" src="images/image17.png"
                            style="width: 315.98px; height: 95.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c43" colspan="1" rowspan="1">
                <p class="c54"><span class="c7">Figure 8</span><span class="c10">: Converting sound data into a
                        mel-spectrogram.</span></p>
            </td>
        </tr>
    </table>
    <p class="c19 c13"><span class="c10">After extracting the mel-spectrogram, we create the time windows from the
            image. For our training, these windows were of size 40x60. Representing 40 mel-frequencies and 60
            milliseconds. This resulted in 13 subimages per sample. This means our model&#39;s input was of shape
            13x40x60x3, the last 3 for the red, green, blue layers.</span></p><a
        id="t.8b326d62446bea822c19c9a68ee5fb813abafb78"></a><a id="t.13"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 333.00px; height: 224.00px;"><img
                            alt="" src="images/image7.png"
                            style="width: 333.00px; height: 224.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 9</span><span class="c10">: A demonstration of how we split the
                        spectrogram into subwindows.</span></p>
            </td>
        </tr>
    </table>
    <h4 class="c38" id="h.m8ijugtu0xaw"><span class="c16">Training</span></h4>
    <p class="c19 c13"><span class="c10">The ConvLSTM2D model trained for 30 epochs using a categorical cross-entropy
            loss and an &lsquo;adam&rsquo; optimizer with 128 batches. Each epoch took around 8 minutes. The model
            started with a 0.25 validation accuracy and reached a 0.84 validation accuracy.</span></p>
    <h3 class="c22" id="h.cjzbnk8cw1vp"><span class="c9">ConvLSTM3D</span></h3><a
        id="t.539638952201acf01dae47eb620a1a33720d7ac4"></a><a id="t.14"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 333.00px; height: 360.00px;"><img
                            alt="" src="images/image14.jpg"
                            style="width: 333.00px; height: 360.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 10</span><span class="c10">: The Convolutional LSTM 3D model
                        architecture. The layers consist of an input layer followed by a Convolutional LSTM with 8
                        features, a Convolutional LSTM with 16 features, a batch normalization, flattening, a fully
                        connected 20, and a fully connected 6 as the output. For both of the ConvLSTM3Ds we used a
                        dropout of 0.2, a randomly generated bias over a normal distribution, and padding
                        &lsquo;same&rsquo;.</span></p>
            </td>
        </tr>
    </table>
    <h4 class="c38" id="h.eqdi9wwqmpdq"><span class="c16">Data Preparation</span></h4>
    <p class="c19 c13"><span>Similarly to the ConvLSTM2D model, we created 2D features vs. time images of the sound.
            Except for the ConvLSTM3D model, we used 4 feature types per sample. We converted the sound recording into
            four images: 1. </span><span class="c10">MFCC constants vs. time 2. Chromagram vs. time, 3. Mel-spectrogram
            vs. time, and 4 Spectral Contrast vs. time. These are shown in figure x. All the features were stretched in
            the y-direction to 60 and cropped in x to 120. Thus each sample has the shape of 4x60x120x3 before splitting
            to sub-images. Where 4 is the number of images, 60 is the height, 120 is the width, and 3 is the RGB.</span>
    </p><a id="t.2c2b14691d63b3cb56e23c83cc45d271719d6d96"></a><a id="t.15"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 310.93px; height: 205.42px;"><img
                            alt="" src="images/image41.png"
                            style="width: 310.93px; height: 205.42px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 11</span><span class="c10">: Converting sound data to feature
                        maps over time. From top to bottom: Mel-Spectrogram, Spectral Contrast, MFCC, and
                        Chromagram.</span></p>
            </td>
        </tr>
    </table>
    <p class="c6 c13"><span class="c10"></span></p>
    <p class="c19 c13"><span class="c10">We split the images similarly to the method used for the ConvLSTM2D as seen in
            figure x, with 60 ms windows. Resulting in windows of size 4x60x60x3. Finally we had 9 subimages per sample.
            So each sample was the size of 9x4x60x60x3. The 9 the number of windows for the LSTM functionality, the 4
            subimages per window are for the 3D convolutions. With this approach several features could be looked at per
            time step to see what emotion was being conveyed in the sound. These time steps would then together
            contribute to the final classification.</span></p>
    <h4 class="c38" id="h.3a1vc65f3wpx"><span class="c16">Training</span></h4>
    <p class="c19 c13"><span>The ConvLSTM2D model trained for 30 epochs using a categorical cross-entropy loss and an
            &lsquo;adam&rsquo; optimizer with 128 batches. Each epoch took around 13 minutes. The model started with a
            0.18 validation accuracy and reached a 0.40 validation accuracy. </span></p>
    <h1 class="c11" id="h.upvwrwq9ivjn"><span class="c7">Results</span></h1>
    <h2 class="c30" id="h.l6exlnaqvcjd"><span>CNN</span></h2><a id="t.0e7829d485d79a266452f40b5316c6f4c4c5fd00"></a><a
        id="t.16"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 316.75px; height: 260.86px;"><img
                            alt="" src="images/image8.png"
                            style="width: 316.75px; height: 260.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.50px; height: 258.15px;"><img
                            alt="" src="images/image31.png"
                            style="width: 312.50px; height: 258.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 12</span><span class="c10">: model accuracy and loss changes on
                        training and testing dataset (trained on 800 epochs). The model started with a 19.11% validation
                        accuracy and reached a 82% +- 1 &nbsp;validation accuracy on testing dataset.</span></p>
            </td>
        </tr>
    </table>
    <p class="c6"><span class="c10"></span></p>
    <p class="c6"><span class="c10"></span></p>
    <p class="c6"><span class="c10"></span></p><a id="t.23074a125251138f7ccf2aa8c76be9d55a43ba93"></a><a id="t.17"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 318.00px; height: 249.33px;"><img
                            alt="" src="images/image30.png"
                            style="width: 318.00px; height: 249.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 318.00px; height: 249.33px;"><img
                            alt="" src="images/image9.png"
                            style="width: 318.00px; height: 249.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 13</span><span class="c10">: a.(upper) confusion matrix on test
                        dataset of 4 dataset combined + data augmentation but without fixing data imbalance on
                        &ldquo;surprised&rdquo; class and b.(lower) confusion matrix after fixing data imbalance on
                        &ldquo;surprised&rdquo; class by augmenting more &ldquo;surprised&rdquo; class data.</span></p>
            </td>
        </tr>
    </table>
    <p class="c6"><span class="c10"></span></p><a id="t.7015d9d81ba873bd50a7db11a1f03edd12a58984"></a><a id="t.18"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.50px; height: 163.00px;"><img
                            alt="" src="images/image23.png"
                            style="width: 312.50px; height: 163.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 313.50px; height: 162.63px;"><img
                            alt="" src="images/image35.png"
                            style="width: 313.50px; height: 162.63px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 14</span><span class="c10">: a.(upper) classification report on
                        validation dataset of 4 dataset combined + data augmentation but without fixing data imbalance
                        on &ldquo;surprised&rdquo; class and b. (lower) classification report after fixing data
                        imbalance on &ldquo;surprised&rdquo; class. Label: &#39;neutral&#39;: 0, &#39;happy&#39;: 1,
                        &#39;sad&#39;: 2, &#39;angry&#39;: 3, &#39;fearful&#39;: 4, &#39;disgust&#39;: 5,
                        &#39;surprised&#39;: 6. On the validation dataset, our model reached an average of 82%+- 1
                        accuracy. We can see that fixing data imbalancing raised precision on predicting
                        &ldquo;surprised&rdquo; class, but did not improve the overall accuracy.</span></p>
            </td>
        </tr>
    </table>
    <h2 class="c28" id="h.xuc483g131q"><span class="c15"></span></h2>
    <h2 class="c30" id="h.oagikfnc8ufh"><span class="c15">Transfer Learning</span></h2>
    <h3 class="c22" id="h.fgxmc7lgyqqt"><span class="c9">Raw VGG16</span></h3><a
        id="t.a138d3c05cdbf25dbafa1dad54a829589288c2a2"></a><a id="t.19"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c37" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 316.00px; height: 344.07px;"><img
                            alt="" src="images/image15.png"
                            style="width: 358.28px; height: 344.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 314.65px; height: 225.83px;"><img
                            alt="" src="images/image1.png"
                            style="width: 320.20px; height: 225.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c25" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 15</span><span class="c10">: Top: Confusion Matrix showing all
                        emotions being classified as disgust and Bottom: model accuracy graph over 50 epochs showing the
                        sporadic accuracies and inability to improve</span></p>
            </td>
        </tr>
    </table>
    <h3 class="c22" id="h.t7pcmp4fzo0l"><span class="c9">First VGG19</span></h3><a
        id="t.896bf505b4813a9fc351dd41414e513d34452406"></a><a id="t.20"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c37" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 309.43px; height: 320.58px;"><img
                            alt="" src="images/image6.png"
                            style="width: 314.31px; height: 320.58px; margin-left: 0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 263.41px; height: 366.48px;"><img
                            alt="" src="images/image20.png"
                            style="width: 263.41px; height: 372.08px; margin-left: -0.00px; margin-top: -5.60px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c25" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 16</span><span class="c10">: Top: Confusion Matrix showing change
                        to favor calm emotion and some ability to classify other emotions. Bottom: Graphs for loss and
                        accuracy show some learning and improvements, but heavy overfitting. </span></p>
            </td>
        </tr>
    </table>
    <h3 class="c22" id="h.r010pdg5fw4x"><span class="c9">Second VGG19</span></h3><a
        id="t.6f9c006212d7c83c345fb4d8fc005561b0c42fb7"></a><a id="t.21"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c49" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 306.50px; height: 158.31px;"><img
                            alt="" src="images/image18.png"
                            style="width: 306.50px; height: 158.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 301.25px; height: 239.48px;"><img
                            alt="" src="images/image32.png"
                            style="width: 301.25px; height: 239.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 247.85px; height: 175.27px;"><img
                            alt="" src="images/image28.png"
                            style="width: 247.85px; height: 175.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 244.00px; height: 173.41px;"><img
                            alt="" src="images/image3.png"
                            style="width: 244.00px; height: 173.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c33" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 17</span><span class="c10">: Top: Confusion Matrix showing much
                        more promising classification results than either previous model. Bottom: Overall accuracy
                        achieved was 45% on 2,603 samples after 15 epochs. Accuracy and loss graphs show good fitting
                        and learning</span></p>
            </td>
        </tr>
    </table>
    <h2 class="c30" id="h.4b8g0n2zqlss"><span>LSTM</span></h2>
    <h3 class="c22" id="h.w9v87kk5trn"><span class="c9">ConvLSTM2D</span></h3><a
        id="t.65d92f8e85529636713fa562ed53a4e75bf461b2"></a><a id="t.22"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 314.50px; height: 153.46px;"><img
                            alt="" src="images/image24.png"
                            style="width: 314.50px; height: 153.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 18</span><span class="c10">: The precision, recall, f1-score, and
                        support of the ConvLSTM2D model split by class, averaged over classes, and weighted averaged.
                        Made from the test subset of the CREMA-D data.</span></p>
            </td>
        </tr>
    </table>
    <p class="c6"><span class="c10"></span></p><a id="t.fe1dc594b556adb207068ff0f25495dd01d403de"></a><a id="t.23"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 304.00px; height: 294.16px;"><img
                            alt="" src="images/image37.png"
                            style="width: 304.00px; height: 294.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 19</span><span class="c10">: The ConvLSTM2D models confusion
                        matrix from the test subset of the CREMA-D data.</span></p>
            </td>
        </tr>
    </table>
    <h3 class="c22" id="h.epv91sq06jbi"><span class="c9">ConvLSTM3D</span></h3><a
        id="t.4ec5b73875e981f1c910629187f6f2084cadc90b"></a><a id="t.24"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 314.00px; height: 164.48px;"><img
                            alt="" src="images/image40.png"
                            style="width: 314.00px; height: 164.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 20</span><span class="c10">: The precision, recall, f1-score, and
                        support of the ConvLSTM3D model split by class, averaged over classes, and weighted averaged.
                        Made from the test subset of the CREMA-D data.</span></p>
            </td>
        </tr>
    </table>
    <p class="c6"><span class="c10"></span></p><a id="t.7711a5c899b3ac6d768d8b348e7f3b97faec9e80"></a><a id="t.25"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 308.61px; height: 299.35px;"><img
                            alt="" src="images/image21.png"
                            style="width: 308.61px; height: 299.35px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 21</span><span class="c10">: The ConvLSTM3D models confusion
                        matrix from the test subset of the CREMA-D data.</span></p>
            </td>
        </tr>
    </table>
    <h1 class="c11" id="h.nivid7vpnilg"><span class="c12">Inference</span></h1>
    <h3 class="c22" id="h.qzi1fzkvvmlo"><span class="c9">Project Architecture</span></h3>
    <p class="c19 c13"><span class="c10">Phase 1: Analyze the sound data. Preprocess the data to implement as a training
            set to several machine learning models. Train several deep learning models on the data. Compare the
            different models to determine which has the highest success.</span></p>
    <p class="c19 c13"><span class="c10">Phase 2: Create a UI for people to interact with the model. Integrate the best
            model into that UI. The UI will take audio input and then will output the detected emotion.</span></p><a
        id="t.d979d9ab8f14ad147292e5b25ed0c24f4313da20"></a><a id="t.26"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c37" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 318.00px; height: 180.00px;"><img
                            alt="" src="images/image5.png"
                            style="width: 318.00px; height: 180.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c25" colspan="1" rowspan="1">
                <p class="c14"><span class="c7">Figure 22</span><span class="c10">: A diagram of our project
                        architecture.</span></p>
            </td>
        </tr>
    </table>
    <h3 class="c22" id="h.pqld0sjw326r"><span class="c9">User Interface(UI):</span></h3>
    <p class="c19 c13"><span class="c10">We picked the CNN model for our Emotion Detection mobile app. We developed a
            Flutter Mobile Application for the user or the customer to interact with the deep learning model. We chose
            flutter because it is a cross-platform framework, meaning it can work on a variety of devices. We also chose
            a mobile application instead of a web application because the input is audio. </span></p>
    <p class="c19 c13"><span class="c10">For this project, we have three screens for the users to interact with. The
            first screen is to record the voice, which then sends the data to the model. The agent then processes the
            audio and provides a response of what emotion is predicted. With that prediction, we go to the next screen
            and print the result with an Emoji and text. Now, the user either needs to click on the green button to
            label the prediction for their voice as right or the red button to label the prediction as wrong. Either of
            those labellings will be stored in a relational database.</span></p>
    <p class="c19 c13"><span class="c10">If the user clicks on the wrong prediction option, they will be redirected to
            choose the right emotion on the screen. The user response is again stored in the same relational
            database.</span></p><a id="t.71472fc4dfb40198e310ceb91133d45f76290663"></a><a id="t.27"></a>
    <table class="c18">
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span
                        style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 2.67px solid #dcddde; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 173.86px; height: 377.95px;"><img
                            alt="" src="images/image19.jpg"
                            style="width: 173.86px; height: 377.95px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 2.67px solid #dcddde; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 173.86px; height: 377.95px;"><img
                            alt="" src="images/image10.jpg"
                            style="width: 173.86px; height: 377.95px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span><span
                        style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 2.67px solid #dcddde; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 173.50px; height: 378.00px;"><img
                            alt="" src="images/image2.jpg"
                            style="width: 173.50px; height: 378.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"
                            title=""></span></p>
            </td>
        </tr>
        <tr class="c3">
            <td class="c23" colspan="1" rowspan="1">
                <p class="c26"><span class="c7">Figure 23</span><span class="c10">: App Screenshots showing the homepage
                        to record the audio, the prediction page, and the page to select the correct emotion if the
                        prediction was incorrect. </span></p>
            </td>
        </tr>
    </table>
    <h1 class="c29" id="h.wgb0yv4ed2sa"><span class="c12">Conclusion and Limitation</span></h1>
    <h2 class="c30" id="h.8zyay6d8hszg"><span class="c15">CNN</span></h2>
    <p class="c19"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Our custom CNN model was first trained on the
            original RAVDESS speech dataset and got 65% accuracy, then we reached 86% +- 1 accuracy on validation
            dataset after augmenting more data. After training on one dataset, we decided to combine all 4 datasets to
            add more data varieties (accents and voice emotion expressions). Due to the limitation of Google Colab Pro
            loader and RAM, after we combined all 4 datasets and augmented more data, we could not train on a dataset
            with 1.3 MB data feature frames of size 210x250. We therefore had to downsize the feature frames by 90x90,
            which brought down the accuracy to 82% +-1. We hypothesized that if we have a better data loader and higher
            RAM to load high quality feature frames, the results can be even better. An advantage of this model is that
            this model can adapt to more voice expressions, song/speech emotions, accent varieties and noise
            environments, and can still make decent predictions with unlimited audio time length. However, as the model
            was trained with audios of only 2-5 seconds, the longer the audio speech in real life, the worse the
            prediction our model can make.</span></p>
    <h2 class="c30" id="h.crx7dg4mxf5i"><span class="c15">Transfer Learning</span></h2>
    <p class="c19"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Transfer learning is a powerful tool to get
            deep neural networks applied to new problems quickly. One of the main limitations is the difference in the
            problem statements from the original training and the new application. In our example, the original training
            for any available model is quite different from our intended use, making the translation of the model to our
            purpose quite difficult to do precisely. For the first two versions of our transfer learning models, we
            stuck with using pretrained weights and only added the finally fully connected layer for our purpose. We
            could do this because we were working with images of standard size with 3 channels, which fits perfectly in
            VGG16 architecture. For the final model, we initialized the same base architecture of VGG16, but needed to
            train from scratch because of the difference in the input shapes, essentially training a very deep neural
            network completely from scratch. We did not have the computational power to complete the needed iterations
            for full training. </span></p>
    <h2 class="c30" id="h.swchz6f3r8g"><span class="c15">LSTM</span></h2>
    <p class="c19 c13"><span>The ConvLSTM2D model did not perform very well on just the mel-spectrogram data as for the
            ConvLSTM3D model performed exceedingly well combining Mel-Spectrogram, Spectral Contrast, MFCC, and
            Chromagram. They arrived at 39 and 97 f1-scores respectively. These results show that the 3D model works
            well for the voices and phrases provided in the CREMA-D dataset and could be generalized to a wider domain
            with more voice samples.</span></p>
    <p class="c19 c13"><span>We experimented in creating these models for sound data, without following any previous
            similar work and therefore were expecting mediocre results. For the ConvLSTM3D, we theorized that the models
            will learn from multiple sound features at once and will make use of the time series quality of the sound
            data to better classify the emotion. The ConvLSTM3D results support our hypotheses and exceed our
            expectations. The models trained for only 30 epochs on the vanilla CREMA-D dataset. We believe that both
            models can be generalized to more data and augmented data to further improve its results on real time voice
            emotion detection.</span></p>
    <h1 class="c11" id="h.o03xnqonm5qb"><span class="c12">Future works</span></h1>
    <p class="c19 c13"><span>First, </span><span class="c10">3 out of 4 datasets that we are using also included videos
            with facial expressions. We can augment the data with the speaker&#39;s face expression frames, taken when
            the speaker records the emotion audio. The face can be treated as one (or multiple) feature frame and can be
            concatenated with the audio feature frames and we can retrain the model again. We expect the accuracy can
            raise better as the model also learns facial expressions and voices to detect emotions.</span></p>
    <p class="c6 c13"><span class="c10"></span></p>
    <p class="c19 c13"><span class="c10">Second, we can first train a model to classify male and female voices before
            detecting their emotions. We hypothesize that males and females will have different emotion expressions and
            their voice changes when expressing emotions may be different (for example, women tend to giggle more than
            men when they are happy), so splitting the training steps into multiple models may increase the prediction
            power of our models.</span></p>
    <p class="c6"><span class="c10"></span></p>
    <p class="c19 c13"><span>Last but not least, in real life, people speak multiple sentences in a short amount of time
            and their emotions can change per sentence. We can use a pretrained Google model to transform the voice to
            words and split the sentences (the model should know when the sentence ends and therefore the audio can be
            split into multiple audios - with each audio being one sentence). Those audios will go to our model to
            predict emotion, so that we can see how people&rsquo;s emotions change per sentence. Another improvement we
            could make to the model is to expand the sound sample time, as longer recordings will give the model more
            input into the person&rsquo;s emotion.</span></p>
    <h1 class="c11 c31" id="h.ff6v8d01x41"><span class="c12"></span></h1>
    <h1 class="c11" id="h.flulmub9f6h4"><span class="c12">References</span></h1>
    <ul class="c35 lst-kix_rt08gvu9v8s5-0 start">
        <li class="c21 c2 li-bullet-0"><span class="c5"><a class="c8"
                    href="https://www.google.com/url?q=https://towardsdatascience.com/self-supervised-voice-emotion-recognition-using-transfer-learning-d21ef7750a10&amp;sa=D&amp;source=editors&amp;ust=1657674612397035&amp;usg=AOvVaw3SfjzonunlSy6sNbkHNAIq">Transfer
                    Learning Tutorial: Goes through data preprocessing, conversion of audio to mel spectrograms, and
                    transfer learning using VGG16</a></span></li>
        <li class="c2 li-bullet-0"><span class="c5"><a class="c8"
                    href="https://www.google.com/url?q=https://www.icloud.com/iclouddrive/04a0_-c5jR_yyOqb9oU-Jq8-Q%23zhou_beigi_2020_2008&amp;sa=D&amp;source=editors&amp;ust=1657674612397483&amp;usg=AOvVaw2BrInJ1sFVFvKTC5Aes88p">A
                    Transfer Learning Method For speech emotion recognition</a></span><span class="c10">&nbsp;</span>
        </li>
        <li class="c21 c2 li-bullet-0"><span class="c5"><a class="c8"
                    href="https://www.google.com/url?q=https://towardsdatascience.com/speech-emotion-recognition-using-ravdess-audio-dataset-ce19d162690&amp;sa=D&amp;source=editors&amp;ust=1657674612397849&amp;usg=AOvVaw3s7NwYsYcIOZRzfe2wKjvP">Another
                    Transfer Learning Tutorial: goes through padding, processing, feature extraction and using several
                    ML models and then a CNN </a></span></li>
        <li class="c21 c2 li-bullet-0"><span class="c5"><a class="c8"
                    href="https://www.google.com/url?q=https://towardsdatascience.com/a-guide-to-convolutional-neural-networks-from-scratch-f1e3bfc3e2de&amp;sa=D&amp;source=editors&amp;ust=1657674612398218&amp;usg=AOvVaw2SFGo10mFyAR0ii4Sz_7Ky">Guide
                    for building a CNN</a></span></li>
        <li class="c2 c21 li-bullet-0"><span class="c5"><a class="c8"
                    href="https://www.google.com/url?q=https://medium.com/swlh/reading-3d-cnn-lstm-deep-neural-networks-for-no-reference-video-quality-assessment-e70359cce64c&amp;sa=D&amp;source=editors&amp;ust=1657674612398590&amp;usg=AOvVaw3xyJIxmJojCA_tHzD1O1VW">ConLSTM3D
                    paper review</a></span></li>
        <li class="c21 c2 li-bullet-0"><span class="c5"><a class="c8"
                    href="https://www.google.com/url?q=https://devopedia.org/audio-feature-extraction%23:~:text%3DAudio%2520features%2520are%2520description%2520of,to%2520build%2520intelligent%2520audio%2520systems.&amp;sa=D&amp;source=editors&amp;ust=1657674612398957&amp;usg=AOvVaw3o0x5aiu0c8fTuk-J_dcR_">Audio
                    Features explain</a></span></li>
    </ul>
    <p class="c1"><span class="c10"></span></p>
    <h1 class="c11" id="h.wmtnsmd08t12"><span class="c12">Appendix</span></h1>
    <h5 class="c4" id="h.9huwzfvc7x02"><span>Project Folder: </span><span class="c5"><a class="c8"
                href="https://www.google.com/url?q=https://drive.google.com/drive/folders/1Ucnfon6JSEjAiuEglT-jCU07noSo0XkF?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1657674612399578&amp;usg=AOvVaw3YUgLFvuM-Bff9x3ZQjONH">EmotionDetector</a></span><span
            class="c0">&nbsp;</span></h5>
    <h5 class="c4" id="h.qgd6clkrt4au"><span>Datasets: </span><span class="c5"><a class="c8"
                href="https://www.google.com/url?q=https://drive.google.com/drive/folders/1UpY26jC_pWuIgWB2pYayFmBKppE3lHOl?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1657674612400037&amp;usg=AOvVaw3O2IMC1-FNjzqgAfDv55qD">Data</a></span>
    </h5>
    <h5 class="c4" id="h.9hcjrlu35uis"><span>Data scraping and preprocessing scripts: </span><span class="c5"><a
                class="c8"
                href="https://www.google.com/url?q=https://drive.google.com/file/d/1PiSnOgZqWrKFKjdJRs33vlYX6w13dDJR/view?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1657674612400445&amp;usg=AOvVaw0z9A3JPk28cRwvETrn-Gfk">Data_Preprocessing.ipynb</a></span>
    </h5>
    <h5 class="c4" id="h.hzqrtvuoj9u"><span>Training model notebooks and API endpoint: </span><span class="c5"><a
                class="c8"
                href="https://www.google.com/url?q=https://drive.google.com/drive/folders/1xtUecDDLDcsNqouHYhszqQ8nGMOewdP0?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1657674612400856&amp;usg=AOvVaw3UbaFpUmVKPRicAkWohboT">Notebooks</a></span>
    </h5>
    <h5 class="c4" id="h.le0llv55x6k4"><span>Saved models: </span><span class="c5"><a class="c8"
                href="https://www.google.com/url?q=https://drive.google.com/drive/folders/1Af8puODsJDI7kmPxP9VfHbAESr_9t8Sc?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1657674612401258&amp;usg=AOvVaw3GBy1g_JGtTv8JoGPQtMsz">Models</a></span>
    </h5>
    <p class="c6"><span class="c10"></span></p>
</body>

</html>